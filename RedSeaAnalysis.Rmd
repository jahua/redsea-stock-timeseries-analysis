---
title: false
output:
  pdf_document:
    toc: false
    number_sections: true
  html_document:
    toc: false
    df_print: paged
header-includes:
- \usepackage{graphicx}
- \usepackage{titling}
- \usepackage{geometry}
- \usepackage{booktabs}
---

\pagenumbering{gobble} <!-- Remove page numbers for title page -->

\begin{titlepage}

\begin{flushright} 
\includegraphics[width=4cm]{assets/hslu_logo.png}
\end{flushright}

\begin{center}
\vspace{2cm}
{\Large Time Series Analysis in Finance\par}

\vspace{2cm}
{\huge\bfseries Impact of Red Sea Crisis on Global Shipping and Stock Markets\par}

\vspace{2cm}
{\Large\bfseries Jiahua Duojie, Thiam Mouhamadou \par}

\vspace{1cm}
{\Large Lucerne University of Applied Sciences and Arts\par}

\vspace{1cm}
{\Large\today\par}

\end{center}
\end{titlepage}

\newpage

\begin{center}
\Large\textbf{Executive Summary}
\end{center}

\vspace{0.5cm}

This research quantifies the financial market impact of the Red Sea shipping crisis that began in November 2023 with Houthi rebel attacks on commercial vessels. Using advanced time series techniques, we analyze how shipping companies' stock prices and risk metrics changed relative to broader market indices.

\vspace{0.25cm}

\textbf{Key Findings:}

\begin{itemize}
  \item \textbf{Volatility:} Shipping stocks experienced 75-120\% volatility increases during the crisis, statistically significant at the 95\% confidence level. GARCH models confirm volatility persistence doubled during the crisis period.
  
  \item \textbf{Market Relationships:} Shipping companies' correlation with broad market indices declined from 0.4-0.6 to 0.1-0.3 during the crisis, while intra-sector correlations strengthened, indicating market segmentation.
  
  \item \textbf{Risk Metrics:} Daily Value-at-Risk (95\%) for shipping stocks worsened by an average of 63\%, with strong statistical evidence that this change was crisis-induced rather than random variation.
  
  \item \textbf{Event Response:} Multiple breakpoint detection methods identify significant structural changes in shipping stock dynamics around the crisis period, with market reactions evolving from initially positive (anticipating higher rates) to negative (reflecting operational challenges).
  
  \item \textbf{Economic Impact:} Estimated \$2.5-3 billion in annual additional shipping costs, 8-day average trip extensions, and 30\% increased fuel consumption due to Cape of Good Hope rerouting.
\end{itemize}

\vspace{0.25cm}

This analysis demonstrates how geopolitical events can create localized financial market effects that differ substantially from broader market movements. The findings provide valuable insight for investors managing shipping exposure, risk managers modeling geopolitical risk, and policymakers quantifying trade disruption costs.

\newpage

\pagenumbering{roman} <!-- Roman numerals for TOC --> \tableofcontents \newpage

\pagenumbering{arabic} <!-- Arabic numerals for main content -->

```{r setup, include=FALSE}
# Load required libraries
library(tidyverse)  # For data manipulation
library(zoo)        # Time series manipulation
library(tseries)    # For stationarity tests
library(vars)       # Vector autoregression
library(quantmod)   # For Yahoo Finance data
library(ggplot2)    # For better visualizations
library(kableExtra) # Better table formatting
library(gridExtra)  # For arranging plots
library(fBasics)    # For comprehensive statistics
library(forecast)   # For ARIMA modeling
library(reshape2)   # For data reshaping
library(lmtest)     # For coeftest
library(knitr)      # For R Markdown options

# Suggest additional packages for enhanced analysis
# These will be used if available, but are not required for core analysis
# install.packages(c("rugarch", "strucchange", "urca", "dygraphs", "highcharter"))

# Set global options
options(scipen = 999) # Turn off scientific notation
knitr::opts_chunk$set(
  echo = TRUE,          # Show code by default
  include = TRUE,       # Show results by default
  warning = FALSE,      # Don't show warnings
  message = FALSE,      # Don't show messages
  fig.align = "center", # Center figures
  fig.width = 8,        # Default figure width
  fig.height = 6,       # Default figure height
  out.width = "90%",    # Control figure size in output
  dpi = 300             # Higher resolution for figures
)

# Function to create consistently formatted tables
formatted_kable <- function(data, caption, digits = 2) {
  kableExtra::kable(data, caption = caption, digits = digits, booktabs = TRUE) %>%
    kableExtra::kable_styling(
      bootstrap_options = c("striped", "hover", "condensed"),
      full_width = FALSE,
      latex_options = c("hold_position")
    ) %>%
    kableExtra::row_spec(0, bold = TRUE) %>%
    kableExtra::column_spec(1, bold = TRUE)
}

# Standard ggplot theme for consistent plots
theme_report <- function() {
  ggplot2::theme_minimal() +
    ggplot2::theme(
      plot.title = ggplot2::element_text(face = "bold", size = 14),
      plot.subtitle = ggplot2::element_text(size = 12, color = "darkgray"),
      axis.title = ggplot2::element_text(face = "bold"),
      legend.position = "bottom",
      legend.title = ggplot2::element_text(face = "bold"),
      panel.grid.minor = ggplot2::element_blank(),
      panel.border = ggplot2::element_rect(fill = NA, color = "lightgray", size = 0.5)
    )
}
```

```{r helper-functions, include=FALSE}
# Function to download financial data with error handling
download_with_error_handling <- function(tickers, start_date, end_date) {
  # Initialize empty list to store results
  data_list <- list()
  
  # Download data for each ticker
  for (ticker in tickers) {
    tryCatch({
      # Download data
      data <- getSymbols(ticker, 
                        from = start_date, 
                        to = end_date, 
                        auto.assign = FALSE)
      
      # Store in list with ticker name
      data_list[[ticker]] <- data
      
    }, error = function(e) {
      warning(paste("Error downloading", ticker, ":", e$message))
    })
  }
  
  return(data_list)
}

# Function to process financial data
process_financial_data <- function(shipping_data, index_data, oil_data, crisis_start) {
  # Combine all data sources into a single list, preserving original ticker names
  all_data_list <- c(shipping_data, index_data, oil_data)
  
  original_ticker_names <- names(all_data_list)

  prices_list <- lapply(original_ticker_names, function(ticker_name) {
    xts_obj <- all_data_list[[ticker_name]]
    
    # Handle cases where data for a ticker might be missing or empty
    if (is.null(xts_obj) || nrow(xts_obj) == 0) {
      warning(paste("Data for ticker", ticker_name, "is NULL or empty. It will be skipped."))
      return(NULL)
    }
    
    tryCatch({
      adj_col <- quantmod::Ad(xts_obj) # Extract Adjusted column
      colnames(adj_col) <- ticker_name  # IMPORTANT: Name the column with the original ticker
      return(adj_col)
    }, error = function(e) {
      warning(paste("Error processing Adjusted prices for ticker", ticker_name, ":", e$message, ". Skipping ticker."))
      return(NULL)
    })
  })
  
  # Filter out any NULLs from tickers that failed to process
  prices_list <- prices_list[!sapply(prices_list, is.null)]
  
  if (length(prices_list) == 0) {
    stop("No valid price data could be processed for any ticker. Check data downloads and ticker symbols.")
  }
  
  # Merge all processed price series
  prices <- do.call(merge, prices_list)
  
  if (nrow(prices) == 0) {
      stop("Resulting merged 'prices' data is empty. Check input data and processing steps.")
  }

  # Calculate returns
  returns <- na.omit(diff(log(prices)))
  
  # Handle cases where returns might be empty (e.g., if prices had only one row)
  if (nrow(returns) == 0 && nrow(prices) > 1) {
      warning("Resulting 'returns' data is empty after diff(log(prices)). Check for NAs or insufficient price data.")
  } else if (nrow(prices) <= 1) {
      warning("Input 'prices' data has one or zero rows, so returns cannot be calculated meaningfully.")
      # Create an empty xts object with a compatible structure for returns
      returns <- xts::xts(matrix(ncol = ncol(prices), nrow = 0), order.by = index(prices)[0])
      if(ncol(prices)>0) colnames(returns) <- colnames(prices)
  }

  # Normalize prices
  normalized_prices <- if (nrow(prices) > 0) {
    prices / as.numeric(prices[1,]) * 100
  } else {
    prices # Return empty if prices is empty, or handle as error
  }
  
  # The column names in `prices` and `returns` are now the original ticker names.
  # We need to identify which of the original shipping/market tickers are present in the final `returns` object,
  # as some might have been skipped due to download/processing errors.
  
  final_shipping_cols <- intersect(names(shipping_data), colnames(returns))
  final_market_cols <- intersect(names(index_data), colnames(returns))
  final_oil_cols <- if (!is.null(oil_data) && length(names(oil_data)) > 0) {
                      intersect(names(oil_data), colnames(returns))
                    } else {
                      character(0) # Empty character vector if oil_data is NULL or has no names
                    }
  
  return(list(
    prices = prices,
    returns = returns,
    normalized_prices = normalized_prices,
    shipping_cols = final_shipping_cols, # Actual columns in 'returns' for shipping
    market_cols = final_market_cols,     # Actual columns in 'returns' for market
    oil_cols = final_oil_cols,           # Actual columns in 'returns' for oil
    all_asset_cols = colnames(returns),  # All columns successfully processed into 'returns'
    crisis_start = crisis_start
  ))
}

# Function to calculate descriptive statistics
calculate_descriptive_stats <- function(returns) {
  stats <- data.frame(
    Mean = colMeans(returns, na.rm = TRUE),
    SD = apply(returns, 2, sd, na.rm = TRUE),
    Min = apply(returns, 2, min, na.rm = TRUE),
    Max = apply(returns, 2, max, na.rm = TRUE),
    Skewness = apply(returns, 2, skewness, na.rm = TRUE),
    Kurtosis = apply(returns, 2, kurtosis, na.rm = TRUE)
  )
  return(stats)
}

# Function to compare pre-crisis and crisis periods
compare_periods <- function(returns, crisis_start) {
  # Convert crisis_start to Date if it's a string
  crisis_date <- as.Date(crisis_start)
  
  # Split data into pre-crisis and crisis periods
  pre_crisis <- returns[index(returns) < crisis_date]
  crisis <- returns[index(returns) >= crisis_date]
  
  # Calculate statistics for each period
  pre_crisis_stats <- calculate_descriptive_stats(pre_crisis)
  crisis_stats <- calculate_descriptive_stats(crisis)
  
  # Create comparison table
  comparison <- data.frame(
    Asset = rownames(pre_crisis_stats),
    Pre_Crisis_Mean = pre_crisis_stats$Mean,
    Crisis_Mean = crisis_stats$Mean,
    Pre_Crisis_SD = pre_crisis_stats$SD,
    Crisis_SD = crisis_stats$SD,
    Pre_Crisis_Min = pre_crisis_stats$Min,
    Crisis_Min = crisis_stats$Min,
    Pre_Crisis_Max = pre_crisis_stats$Max,
    Crisis_Max = crisis_stats$Max,
    Pre_Crisis_Skew = pre_crisis_stats$Skewness,
    Crisis_Skew = crisis_stats$Skewness,
    Pre_Crisis_Kurt = pre_crisis_stats$Kurtosis,
    Crisis_Kurt = crisis_stats$Kurtosis
  )
  
  # Calculate changes
  comparison$Mean_Change <- comparison$Crisis_Mean - comparison$Pre_Crisis_Mean
  comparison$SD_Change <- comparison$Crisis_SD - comparison$Pre_Crisis_SD
  
  return(comparison)
}

# Function to plot normalized prices
plot_prices <- function(normalized_prices, tickers, crisis_start, title) {
  # Convert to data frame for ggplot
  plot_data <- as.data.frame(normalized_prices)
  plot_data$Date <- index(normalized_prices)
  
  # Get actual column names from the data
  available_cols <- colnames(plot_data)[-ncol(plot_data)]  # Exclude Date column
  
  # Find matching columns for each ticker
  matching_cols <- sapply(tickers, function(ticker) {
    # Try exact match first
    if (ticker %in% available_cols) {
      return(ticker)
    }
    # Try matching without exchange suffix
    base_ticker <- gsub("\\..*$", "", ticker)
    if (base_ticker %in% available_cols) {
      return(base_ticker)
    }
    # Try matching with common suffixes removed
    clean_ticker <- gsub("[-.]", "", ticker)
    clean_cols <- gsub("[-.]", "", available_cols)
    match_idx <- which(clean_cols == clean_ticker)
    if (length(match_idx) > 0) {
      return(available_cols[match_idx[1]])
    }
    return(NULL)
  })
  
  # Filter out NULL values
  matching_cols <- matching_cols[!sapply(matching_cols, is.null)]
  
  if (length(matching_cols) == 0) {
    stop("No matching columns found for the given tickers")
  }
  
  # Reshape data for plotting
  plot_data_long <- reshape2::melt(plot_data, id.vars = "Date", 
                                 measure.vars = matching_cols,
                                 variable.name = "Asset",
                                 value.name = "Price")
  
  # Create plot
  p <- ggplot(plot_data_long, aes(x = Date, y = Price, color = Asset)) +
    geom_line() +
    geom_vline(xintercept = as.Date(crisis_start), 
               linetype = "dashed", color = "red") +
    labs(title = title,
         x = "Date",
         y = "Normalized Price (100 = Start)",
         color = "Asset") +
    theme_minimal() +
    theme(legend.position = "bottom") +
    scale_color_brewer(palette = "Set1")
  
  return(p)
}

# Function to perform enhanced stationarity tests
enhanced_stationarity_tests <- function(returns) {
  # Initialize results data frame
  results <- data.frame(
    Asset = character(),
    ADF_Stat = numeric(),
    ADF_pval = numeric(),
    KPSS_Stat = numeric(),
    KPSS_pval = numeric(),
    PP_Stat = numeric(),
    PP_pval = numeric()
  )
  
  # Perform tests for each column
  for (col in colnames(returns)) {
    # Augmented Dickey-Fuller test
    adf_test <- adf.test(returns[, col], alternative = "stationary")
    
    # KPSS test
    kpss_test <- kpss.test(returns[, col], null = "Level")
    
    # Phillips-Perron test
    pp_test <- pp.test(returns[, col], alternative = "stationary")
    
    # Add results
    results <- rbind(results, data.frame(
      Asset = col,
      ADF_Stat = adf_test$statistic,
      ADF_pval = adf_test$p.value,
      KPSS_Stat = kpss_test$statistic,
      KPSS_pval = kpss_test$p.value,
      PP_Stat = pp_test$statistic,
      PP_pval = pp_test$p.value
    ))
  }
  
  # Add interpretation
  results$ADF_Interpretation <- ifelse(results$ADF_pval < 0.05, "Stationary", "Non-stationary")
  results$KPSS_Interpretation <- ifelse(results$KPSS_pval > 0.05, "Stationary", "Non-stationary")
  results$PP_Interpretation <- ifelse(results$PP_pval < 0.05, "Stationary", "Non-stationary")
  
  # Add overall conclusion
  results$Overall_Conclusion <- ifelse(
    (results$ADF_pval < 0.05 & results$KPSS_pval > 0.05 & results$PP_pval < 0.05),
    "Stationary",
    "Non-stationary"
  )
  
  return(results)
}

# Function to create correlation heatmap
create_correlation_heatmap <- function(returns, period = "all", crisis_start = NULL) {
  # Filter data based on period if specified
  if (period != "all" && !is.null(crisis_start)) {
    crisis_date <- as.Date(crisis_start)
    if (period == "pre_crisis") {
      returns <- returns[index(returns) < crisis_date]
    } else if (period == "crisis") {
      returns <- returns[index(returns) >= crisis_date]
    }
  }
  
  # Calculate correlation matrix
  cor_matrix <- cor(returns, use = "pairwise.complete.obs")
  
  # Convert to long format for ggplot
  cor_data <- reshape2::melt(cor_matrix)
  
  # Create heatmap
  p <- ggplot(cor_data, aes(x = Var1, y = Var2, fill = value)) +
    geom_tile() +
    scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                        midpoint = 0, limit = c(-1, 1), space = "Lab",
                        name = "Correlation") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1),
          axis.title = element_blank()) +
    coord_fixed() +
    labs(title = paste("Correlation Heatmap -", 
                      ifelse(period == "all", "Full Period",
                            ifelse(period == "pre_crisis", "Pre-Crisis Period",
                                  "Crisis Period"))))
  
  return(p)
}

# Function to analyze correlation changes
analyze_correlation_changes <- function(returns, crisis_start) {
  # Convert crisis_start to Date if it's a string
  crisis_date <- as.Date(crisis_start)
  
  # Split data into pre-crisis and crisis periods
  pre_crisis <- returns[index(returns) < crisis_date]
  crisis <- returns[index(returns) >= crisis_date]
  
  # Calculate correlation matrices
  pre_crisis_cor <- cor(pre_crisis, use = "pairwise.complete.obs")
  crisis_cor <- cor(crisis, use = "pairwise.complete.obs")
  
  # Calculate correlation changes
  cor_changes <- crisis_cor - pre_crisis_cor
  
  # Convert to long format for plotting
  cor_changes_long <- reshape2::melt(cor_changes)
  
  # Create heatmap of correlation changes
  p <- ggplot(cor_changes_long, aes(x = Var1, y = Var2, fill = value)) +
    geom_tile() +
    scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                        midpoint = 0, limit = c(-1, 1), space = "Lab",
                        name = "Correlation\nChange") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1),
          axis.title = element_blank()) +
    coord_fixed() +
    labs(title = "Changes in Correlation During Crisis")
  
  # Calculate summary statistics for changes
  mean_change <- mean(cor_changes[upper.tri(cor_changes)], na.rm = TRUE)
  sd_change <- sd(cor_changes[upper.tri(cor_changes)], na.rm = TRUE)
  max_increase <- max(cor_changes[upper.tri(cor_changes)], na.rm = TRUE)
  max_decrease <- min(cor_changes[upper.tri(cor_changes)], na.rm = TRUE)
  pct_increase <- mean(cor_changes[upper.tri(cor_changes)] > 0, na.rm = TRUE) * 100
  
  summary_stats <- data.frame(
    Mean_Change = mean_change,
    SD_Change = sd_change,
    Max_Increase = max_increase,
    Max_Decrease = max_decrease,
    Pct_Increase = pct_increase
  )
  
  # Return the results
  return(list(
    plot = p,
    summary = summary_stats,
    changes = cor_changes
  ))
}

# Function to define key Red Sea crisis events
define_red_sea_events <- function() {
  events <- data.frame(
    Date = as.Date(c(
      "2023-11-19",  # First Houthi attack on commercial vessel
      "2023-12-18",  # Major shipping companies announce route changes
      "2024-01-12",  # US and UK launch strikes against Houthi targets
      "2024-01-16",  # Maersk announces temporary suspension of Red Sea transits
      "2024-02-14",  # Houthis attack bulk carrier
      "2024-03-06"   # Major shipping companies announce extended route changes
    )),
    Event = c(
      "First Houthi Attack",
      "Major Route Changes",
      "US-UK Military Response",
      "Maersk Suspension",
      "Bulk Carrier Attack",
      "Extended Route Changes"
    ),
    Description = c(
      "First attack on commercial vessel in Red Sea",
      "Multiple shipping companies announce rerouting around Cape of Good Hope",
      "US and UK launch military strikes against Houthi targets",
      "Maersk announces temporary suspension of Red Sea transits",
      "Houthis attack bulk carrier in Red Sea",
      "Shipping companies announce extended route changes due to ongoing threats"
    )
  )
  
  return(events)
}

# Function to calculate event returns
calculate_event_returns <- function(returns, events, market_index = NULL, window = 20) {
  # Ensure events$Date is Date type
  events$Date <- as.Date(events$Date)
  
  # Prepare result data frame
  result <- data.frame(Event = events$Event, Date = events$Date)
  
  # For each asset, calculate cumulative return after each event
  for (asset in colnames(returns)) {
    cum_returns <- sapply(events$Date, function(event_date) {
      idx <- which(index(returns) == event_date)
      if (length(idx) == 0 || (idx + window) > nrow(returns)) {
        return(NA)
      }
      # Cumulative return over the window
      sum(returns[(idx+1):(idx+window), asset], na.rm = TRUE) * 100
    })
    result[[paste0(asset, "_cum_return")]] <- cum_returns
  }
  
  # If market_index is provided, calculate abnormal returns
  if (!is.null(market_index) && market_index %in% colnames(returns)) {
    for (asset in colnames(returns)) {
      if (asset == market_index) next
      abn_returns <- sapply(events$Date, function(event_date) {
        idx <- which(index(returns) == event_date)
        if (length(idx) == 0 || (idx + window) > nrow(returns)) {
          return(NA)
        }
        asset_cum <- sum(returns[(idx+1):(idx+window), asset], na.rm = TRUE)
        market_cum <- sum(returns[(idx+1):(idx+window), market_index], na.rm = TRUE)
        (asset_cum - market_cum) * 100
      })
      result[[paste0(asset, "_abn_return")]] <- abn_returns
    }
  }
  
  return(result)
}

# Function to plot event impact
plot_event_impact <- function(event_returns, asset_names, return_type = "cumulative") {
  # Ensure required packages are available
  if (!requireNamespace("tidyr", quietly = TRUE)) {
    stop("Package 'tidyr' is needed. Please install it.", call. = FALSE)
  }
  if (!requireNamespace("dplyr", quietly = TRUE)) {
    stop("Package 'dplyr' is needed. Please install it.", call. = FALSE)
  }
  if (!requireNamespace("ggplot2", quietly = TRUE)) {
    stop("Package 'ggplot2' is needed. Please install it.", call. = FALSE)
  }

  # Determine column suffix and plot labels based on return_type
  suffix <- if (return_type == "cumulative") "_cum_return" else "_abn_return"
  plot_title <- if (return_type == "cumulative") {
    "Event Impact (20-Day Cumulative Returns)"
  } else {
    "Event Impact (20-Day Abnormal Returns)"
  }
  y_label <- if (return_type == "cumulative") "Cumulative Return (%)" else "Abnormal Return (%)"

  # Construct the full column names to select from event_returns
  target_cols_to_plot <- paste0(asset_names, suffix)

  # Identify which of these target columns actually exist in the event_returns dataframe
  existing_target_cols <- intersect(target_cols_to_plot, colnames(event_returns))

  if (length(existing_target_cols) == 0) {
    msg <- paste("No columns found in event_returns for assets:", 
                 paste(asset_names, collapse=", "), 
                 "with suffix:", suffix, ".",
                 "\nAvailable columns containing '_return':", 
                 paste(grep("_return", colnames(event_returns), value=TRUE), collapse=", "))
    stop(msg, call. = FALSE)
  }
  
  # Select 'Event', 'Date', and the identified existing target columns for plotting
  plot_data <- event_returns %>%
    dplyr::select(dplyr::all_of(c("Event", "Date", existing_target_cols)))

  # Reshape data to long format for ggplot
  plot_data_long <- plot_data %>%
    tidyr::pivot_longer(cols = dplyr::all_of(existing_target_cols),
                 names_to = "AssetFull", 
                 values_to = "Return_Value") %>%
    dplyr::mutate(Asset = gsub(suffix, "", AssetFull)) # Clean asset names for the legend

  # Ensure 'Event' is a factor to maintain the order from the original events data frame
  if ("Event" %in% names(event_returns)) {
      plot_data_long$Event <- factor(plot_data_long$Event, levels = unique(event_returns$Event))
  }


  # Create the bar chart
  p <- ggplot2::ggplot(plot_data_long, ggplot2::aes(x = Event, y = Return_Value, fill = Asset)) +
    ggplot2::geom_bar(stat = "identity", position = ggplot2::position_dodge(width = 0.9)) +
    ggplot2::labs(title = plot_title,
         x = "Event",
         y = y_label,
         fill = "Asset") +
    ggplot2::theme_minimal(base_size = 11) + # Adjusted base_size for better fit
    ggplot2::theme(axis.text.x = ggplot2::element_text(angle = 45, hjust = 1, vjust = 1), # Ensure labels don't overlap
                   legend.position = "bottom",
                   plot.title = ggplot2::element_text(hjust = 0.5)) + # Center plot title
    ggplot2::scale_fill_brewer(palette = "Set2")
  
  return(p)
}

# Function to calculate and compare VaR for different periods
compare_var_periods <- function(returns, asset_cols, crisis_start, confidence_level = 0.95) {
  # Ensure required packages are available
  if (!requireNamespace("dplyr", quietly = TRUE)) {
    stop("Package 'dplyr' is needed. Please install it.", call. = FALSE)
  }

  crisis_date <- as.Date(crisis_start)
  
  # Split data
  pre_crisis_returns <- returns[index(returns) < crisis_date, asset_cols, drop = FALSE]
  crisis_returns <- returns[index(returns) >= crisis_date, asset_cols, drop = FALSE]
  
  # Calculate VaR for pre-crisis period
  pre_crisis_var <- apply(pre_crisis_returns, 2, function(x) {
    quantile(x, probs = (1 - confidence_level), na.rm = TRUE) * 100 # VaR as percentage
  })
  
  # Calculate VaR for crisis period
  crisis_var <- apply(crisis_returns, 2, function(x) {
    quantile(x, probs = (1 - confidence_level), na.rm = TRUE) * 100 # VaR as percentage
  })
  
  # Combine results
  var_comparison <- data.frame(
    Asset = asset_cols,
    Pre_Crisis_VaR = pre_crisis_var[match(asset_cols, names(pre_crisis_var))],
    Crisis_VaR = crisis_var[match(asset_cols, names(crisis_var))]
  )
  
  # Calculate change in VaR
  var_comparison <- var_comparison %>%
    dplyr::mutate(VaR_Change = Crisis_VaR - Pre_Crisis_VaR,
                  VaR_Pct_Change = ifelse(Pre_Crisis_VaR != 0, (VaR_Change / abs(Pre_Crisis_VaR)) * 100, NA)) # Avoid division by zero
  
  rownames(var_comparison) <- NULL
  return(var_comparison)
}

# Function to plot VaR comparison
plot_var_comparison <- function(var_results) {
  # Ensure required packages are available
  if (!requireNamespace("tidyr", quietly = TRUE)) {
    stop("Package 'tidyr' is needed. Please install it.", call. = FALSE)
  }
  if (!requireNamespace("ggplot2", quietly = TRUE)) {
    stop("Package 'ggplot2' is needed. Please install it.", call. = FALSE)
  }
  
  # Reshape data for plotting
  plot_data <- var_results %>%
    tidyr::pivot_longer(
      cols = c("Pre_Crisis_VaR", "Crisis_VaR"),
      names_to = "Period",
      values_to = "VaR"
    ) %>%
    dplyr::mutate(Period = factor(Period, 
                                  levels = c("Pre_Crisis_VaR", "Crisis_VaR"),
                                  labels = c("Pre-Crisis", "Crisis")))
  
  # Create the plot
  p <- ggplot2::ggplot(plot_data, ggplot2::aes(x = Asset, y = VaR, fill = Period)) +
    ggplot2::geom_bar(stat = "identity", position = ggplot2::position_dodge()) +
    ggplot2::geom_hline(yintercept = 0, linetype = "dashed", color = "darkgray") +
    ggplot2::labs(
      title = "Value at Risk (VaR) Comparison",
      subtitle = "95% Confidence Level, Daily Returns (%)",
      x = "Asset",
      y = "Value at Risk (%)",
      fill = "Period"
    ) +
    ggplot2::theme_minimal() +
    ggplot2::theme(
      axis.text.x = ggplot2::element_text(angle = 45, hjust = 1),
      legend.position = "bottom"
    ) +
    ggplot2::scale_fill_manual(values = c("Pre-Crisis" = "steelblue", "Crisis" = "firebrick"))
  
  return(p)
}

# Function to analyze VaR changes
analyze_var_changes <- function(var_results) {
  # Ensure required packages are available
  if (!requireNamespace("dplyr", quietly = TRUE)) {
    stop("Package 'dplyr' is needed. Please install it.", call. = FALSE)
  }
  
  # Calculate summary statistics for VaR changes
  summary_stats <- data.frame(
    Mean_VaR_Change = mean(var_results$VaR_Change, na.rm = TRUE),
    Median_VaR_Change = median(var_results$VaR_Change, na.rm = TRUE),
    Max_VaR_Worsening = min(var_results$VaR_Change, na.rm = TRUE), # VaR is negative, so worsening is min
    Max_VaR_Improvement = max(var_results$VaR_Change, na.rm = TRUE),
    Mean_Pct_Change = mean(var_results$VaR_Pct_Change, na.rm = TRUE),
    Assets_With_Increased_Risk = sum(var_results$VaR_Change < 0, na.rm = TRUE),
    Total_Assets = nrow(var_results),
    Pct_Assets_With_Increased_Risk = sum(var_results$VaR_Change < 0, na.rm = TRUE) / nrow(var_results) * 100
  )
  
  # Categorize assets by risk change
  var_categories <- var_results %>%
    dplyr::mutate(
      Risk_Change_Category = dplyr::case_when(
        VaR_Change < -1 ~ "Significantly Increased Risk",
        VaR_Change < 0 ~ "Slightly Increased Risk",
        VaR_Change > 1 ~ "Significantly Decreased Risk",
        TRUE ~ "Slightly Decreased Risk"
      )
    ) %>%
    dplyr::group_by(Risk_Change_Category) %>%
    dplyr::summarize(Count = dplyr::n(), Assets = paste(Asset, collapse = ", "))
  
  return(list(
    summary = summary_stats,
    categories = var_categories
  ))
}

# Function to test VaR changes statistically
test_var_changes <- function(var_changes) {
  # Check if var_changes is NULL or doesn't have the expected structure
  if (is.null(var_changes) || !is.list(var_changes) || !("summary" %in% names(var_changes))) {
    warning("Invalid var_changes object. Expected a list with 'summary' element.")
    return(data.frame(
      Note = "Invalid var_changes object",
      stringsAsFactors = FALSE
    ))
  }
  
  # Extract summary statistics
  summary_stats <- var_changes$summary
  
  # Check if categories exist and have enough data
  if (!("categories" %in% names(var_changes)) || 
      is.null(var_changes$categories) || 
      nrow(var_changes$categories) <= 1) {
    return(data.frame(
      Note = "Insufficient data for statistical testing of VaR changes",
      stringsAsFactors = FALSE
    ))
  }
  
  # Initialize results
  t_test_results <- data.frame(
    Test = character(0),
    Statistic = numeric(0),
    P_Value = numeric(0),
    Significant = logical(0),
    stringsAsFactors = FALSE
  )
  
  # Prepare data for t-test (try to extract relevant numeric data)
  var_change_values <- tryCatch({
    if ("VaR_Change" %in% names(var_changes$categories)) {
      var_changes$categories$VaR_Change
    } else if ("Count" %in% names(var_changes$categories)) {
      var_changes$categories$Count
    } else {
      # Try to find any numeric column
      numeric_cols <- sapply(var_changes$categories, is.numeric)
      if (any(numeric_cols)) {
        var_changes$categories[[which(numeric_cols)[1]]]
      } else {
        NULL
      }
    }
  }, error = function(e) {
    warning("Error extracting numeric data from var_changes: ", e$message)
    return(NULL)
  })
  
  # Check if we have valid data for the t-test
  if (is.null(var_change_values) || length(var_change_values) < 2) {
    return(data.frame(
      Note = "Could not extract sufficient numeric data for statistical testing",
      stringsAsFactors = FALSE
    ))
  }
  
  # Perform t-test with error handling
  t_result <- tryCatch({
    t.test(var_change_values)
  }, error = function(e) {
    warning("Error in t-test for VaR changes: ", e$message)
    return(NULL)
  })
  
  # Return appropriate results based on t-test outcome
  if (is.null(t_result)) {
    return(data.frame(
      Note = "T-test failed for VaR changes",
      stringsAsFactors = FALSE
    ))
  } else {
    # Create result row
    result_row <- data.frame(
      Test = "T-test for VaR Change",
      Statistic = t_result$statistic,
      P_Value = t_result$p.value,
      Significant = t_result$p.value < 0.05,
      stringsAsFactors = FALSE
    )
    
    return(result_row)
  }
}

# Function to run VAR analysis
run_var_analysis <- function(var_data, max_lag = 8) {
  # Ensure required packages are available
  if (!requireNamespace("vars", quietly = TRUE)) {
    stop("Package 'vars' is needed. Please install it.", call. = FALSE)
  }
  
  # For simplicity and robustness, just use a fixed lag of 1
  # This avoids all the issues with trying to determine optimal lag
  p <- 1
  
  # Fit VAR model with the fixed lag
  var_model <- vars::VAR(var_data, p = p, type = "const")
  
  # Calculate Granger causality
  granger_results <- list()
  for (col in colnames(var_data)) {
    tryCatch({
      granger_results[[col]] <- vars::causality(var_model, cause = col)$Granger
    }, error = function(e) {
      warning("Error calculating Granger causality for ", col, ": ", e$message)
    })
  }
  
  # Run impulse response analysis
  irf_results <- tryCatch({
    vars::irf(var_model, n.ahead = 10)
  }, error = function(e) {
    warning("Error calculating impulse response: ", e$message)
    NULL
  })
  
  # Return results
  return(list(
    model = var_model,
    granger = granger_results,
    irf = irf_results,
    summary = summary(var_model)
  ))
}

# Function to generate VAR forecasts
generate_var_forecasts <- function(var_model, n.ahead = 20) {
  # Ensure required packages are available
  if (!requireNamespace("vars", quietly = TRUE)) {
    stop("Package 'vars' is needed. Please install it.", call. = FALSE)
  }
  if (!requireNamespace("ggplot2", quietly = TRUE)) {
    stop("Package 'ggplot2' is needed. Please install it.", call. = FALSE)
  }
  
  # Generate forecasts - use predict() as a method, not from vars namespace
  var_forecasts <- predict(var_model, n.ahead = n.ahead)
  
  # Create plots
  plots <- list()
  for (series in names(var_forecasts$fcst)) {
    # Extract forecast data for this series
    fcst_data <- var_forecasts$fcst[[series]]
    
    # Convert to data frame for ggplot
    df <- data.frame(
      h = 1:n.ahead,
      forecast = fcst_data[, 1],
      lower_ci = fcst_data[, 2],
      upper_ci = fcst_data[, 3]
    )
    
    # Create plot
    p <- ggplot2::ggplot(df, ggplot2::aes(x = h)) +
      ggplot2::geom_line(ggplot2::aes(y = forecast), color = "blue") +
      ggplot2::geom_ribbon(ggplot2::aes(ymin = lower_ci, ymax = upper_ci), fill = "blue", alpha = 0.2) +
      ggplot2::geom_hline(yintercept = 0, linetype = "dashed", color = "gray") +
      ggplot2::labs(
        title = paste("VAR Forecast for", series),
        x = "Horizon (Days)",
        y = "Forecasted Returns"
      ) +
      ggplot2::theme_minimal()
    
    plots[[series]] <- p
  }
  
  # Create impulse response plots
  irf_results <- vars::irf(var_model, n.ahead = n.ahead)
  irf_plots <- plot(irf_results)
  
  return(list(
    forecasts = var_forecasts,
    plots = plots,
    irf_plots = irf_plots
  ))
}

# Function to validate forecasts
validate_forecasts <- function(returns, var_model, test_window = 20) {
  # Ensure required packages are available
  if (!requireNamespace("vars", quietly = TRUE)) {
    stop("Package 'vars' is needed. Please install it.", call. = FALSE)
  }
  
  # Extract recent data for validation
  n <- nrow(returns)
  train_data <- returns[1:(n - test_window), ]
  test_data <- returns[(n - test_window + 1):n, ]
  
  # Refit VAR model on training data
  validation_model <- vars::VAR(train_data, p = var_model$p, type = "const")
  
  # Generate forecasts - use predict() as a method, not from vars namespace
  validation_forecasts <- predict(validation_model, n.ahead = test_window)
  
  # Calculate error metrics for each series
  error_metrics <- data.frame(
    Asset = character(0),
    MAE = numeric(0),
    RMSE = numeric(0),
    MAPE = numeric(0),
    Theil_U = numeric(0)
  )
  
  for (series in colnames(returns)) {
    forecasts <- validation_forecasts$fcst[[series]][, 1]
    actuals <- as.numeric(test_data[, series])
    
    # Calculate metrics
    mae <- mean(abs(forecasts - actuals), na.rm = TRUE)
    rmse <- sqrt(mean((forecasts - actuals)^2, na.rm = TRUE))
    mape <- mean(abs((forecasts - actuals) / actuals), na.rm = TRUE) * 100
    theil_u <- sqrt(sum((forecasts - actuals)^2, na.rm = TRUE)) / 
              sqrt(sum(actuals^2, na.rm = TRUE))
    
    # Add to results
    error_metrics <- rbind(error_metrics, data.frame(
      Asset = series,
      MAE = mae,
      RMSE = rmse,
      MAPE = mape,
      Theil_U = theil_u
    ))
  }
  
  return(error_metrics)
}

# Function to forecast with ARIMA
forecast_with_arima <- function(price_series, ticker, h = 30) {
  # Ensure required packages are available
  if (!requireNamespace("forecast", quietly = TRUE)) {
    stop("Package 'forecast' is needed. Please install it.", call. = FALSE)
  }
  if (!requireNamespace("ggplot2", quietly = TRUE)) {
    stop("Package 'ggplot2' is needed. Please install it.", call. = FALSE)
  }
  
  # Convert xts to ts
  price_ts <- as.ts(price_series)
  
  # Fit auto ARIMA model
  arima_model <- forecast::auto.arima(price_ts, seasonal = FALSE)
  
  # Generate forecast
  arima_forecast <- forecast::forecast(arima_model, h = h)
  
  # Convert forecast to data frame for ggplot
  forecast_df <- data.frame(
    Date = as.Date(seq(from = index(price_series)[length(index(price_series))], 
                       by = "day", length.out = h + 1)[-1]),
    Forecast = as.numeric(arima_forecast$mean),
    Lower_80 = as.numeric(arima_forecast$lower[, 1]),
    Upper_80 = as.numeric(arima_forecast$upper[, 1]),
    Lower_95 = as.numeric(arima_forecast$lower[, 2]),
    Upper_95 = as.numeric(arima_forecast$upper[, 2])
  )
  
  # Historical dates and values for plotting
  historical_df <- data.frame(
    Date = index(price_series),
    Value = as.numeric(price_series)
  )
  
  # Create plot
  p <- ggplot2::ggplot() +
    ggplot2::geom_line(data = historical_df, ggplot2::aes(x = Date, y = Value), color = "black") +
    ggplot2::geom_line(data = forecast_df, ggplot2::aes(x = Date, y = Forecast), color = "blue") +
    ggplot2::geom_ribbon(data = forecast_df, 
                ggplot2::aes(x = Date, ymin = Lower_95, ymax = Upper_95), 
                fill = "blue", alpha = 0.2) +
    ggplot2::labs(
      title = paste("ARIMA Forecast for", ticker),
      x = "Date",
      y = "Price"
    ) +
    ggplot2::theme_minimal()
  
  return(list(
    model = arima_model,
    forecast = arima_forecast,
    plot = p
  ))
}

# Function to calculate rolling beta
calculate_rolling_beta <- function(returns, market_col, window = 60) {
  # Ensure required packages are available
  if (!requireNamespace("zoo", quietly = TRUE)) {
    stop("Package 'zoo' is needed. Please install it.", call. = FALSE)
  }
  
  # Initialize results data frame
  rolling_betas <- data.frame(
    Date = index(returns)[(window):nrow(returns)]
  )
  
  # Calculate rolling beta for each asset against the market
  for (col in colnames(returns)) {
    if (col == market_col) next  # Skip market itself
    
    # Calculate rolling beta
    betas <- sapply((window):nrow(returns), function(i) {
      window_data <- returns[(i - window + 1):i, ]
      model <- lm(window_data[, col] ~ window_data[, market_col])
      return(coef(model)[2])  # Beta is the slope coefficient
    })
    
    # Add to results
    rolling_betas[[paste0(col, "_beta")]] <- betas
  }
  
  # Convert to xts for easier time series operations
  rolling_betas_xts <- xts::xts(rolling_betas[, -1], order.by = as.Date(rolling_betas$Date))
  
  return(rolling_betas_xts)
}

# Function to plot rolling betas
plot_rolling_betas <- function(rolling_betas, crisis_start) {
  # Ensure required packages are available
  if (!requireNamespace("ggplot2", quietly = TRUE)) {
    stop("Package 'ggplot2' is needed. Please install it.", call. = FALSE)
  }
  if (!requireNamespace("reshape2", quietly = TRUE)) {
    stop("Package 'reshape2' is needed. Please install it.", call. = FALSE)
  }
  
  # Convert to data frame for ggplot
  plot_data <- data.frame(
    Date = index(rolling_betas),
    as.data.frame(rolling_betas)
  )
  
  # Reshape to long format
  plot_data_long <- reshape2::melt(plot_data, id.vars = "Date",
                                  variable.name = "Asset",
                                  value.name = "Beta")
  
  # Clean asset names (remove _beta suffix)
  plot_data_long$Asset <- gsub("_beta$", "", plot_data_long$Asset)
  
  # Create plot
  p <- ggplot2::ggplot(plot_data_long, ggplot2::aes(x = Date, y = Beta, color = Asset)) +
    ggplot2::geom_line() +
    ggplot2::geom_hline(yintercept = 1, linetype = "dashed", color = "gray") +
    ggplot2::geom_hline(yintercept = 0, linetype = "dotted", color = "gray") +
    ggplot2::geom_vline(xintercept = as.Date(crisis_start), 
               linetype = "dashed", color = "red") +
    ggplot2::labs(
      title = "Rolling Beta to Market",
      x = "Date",
      y = "Beta Coefficient",
      color = "Asset"
    ) +
    ggplot2::theme_minimal() +
    ggplot2::theme(legend.position = "bottom") +
    ggplot2::scale_color_brewer(palette = "Set1")
  
  return(p)
}

# Function to analyze beta changes
analyze_beta_changes <- function(rolling_betas, crisis_start) {
  # Ensure crisis_start is a Date
  crisis_date <- as.Date(crisis_start)
  
  # Split data
  pre_crisis <- rolling_betas[index(rolling_betas) < crisis_date]
  crisis <- rolling_betas[index(rolling_betas) >= crisis_date]
  
  # Calculate statistics for each asset
  results <- data.frame(
    Asset = character(),
    Pre_Crisis_Mean_Beta = numeric(),
    Crisis_Mean_Beta = numeric(),
    Beta_Change = numeric(),
    Pre_Crisis_Vol = numeric(),
    Crisis_Vol = numeric(),
    Vol_Change = numeric()
  )
  
  # Process each asset's beta
  for (col in colnames(rolling_betas)) {
    # Calculate statistics
    pre_mean <- mean(pre_crisis[, col], na.rm = TRUE)
    crisis_mean <- mean(crisis[, col], na.rm = TRUE)
    beta_change <- crisis_mean - pre_mean
    
    pre_vol <- sd(pre_crisis[, col], na.rm = TRUE)
    crisis_vol <- sd(crisis[, col], na.rm = TRUE)
    vol_change <- crisis_vol / pre_vol - 1
    
    # Add to results
    results <- rbind(results, data.frame(
      Asset = gsub("_beta$", "", col),
      Pre_Crisis_Mean_Beta = pre_mean,
      Crisis_Mean_Beta = crisis_mean,
      Beta_Change = beta_change,
      Pre_Crisis_Vol = pre_vol,
      Crisis_Vol = crisis_vol,
      Vol_Change = vol_change
    ))
  }
  
  return(results)
}

# Function to test volatility changes
test_volatility_changes <- function(returns, asset_cols, crisis_date) {
  results <- data.frame(
    Asset = character(),
    Pre_Crisis_Vol = numeric(),
    Crisis_Vol = numeric(),
    Change_Pct = numeric(),
    F_Stat = numeric(),
    P_Value = numeric(),
    Significant = logical(),
    stringsAsFactors = FALSE
  )

  for (col in asset_cols) {
    if (col %in% colnames(returns)) {
      pre_vals <- as.numeric(returns[index(returns) < crisis_date, col])
      post_vals <- as.numeric(returns[index(returns) >= crisis_date, col])

      pre_vol <- sd(pre_vals, na.rm = TRUE)
      post_vol <- sd(post_vals, na.rm = TRUE)
      vol_change_pct <- (post_vol / pre_vol - 1) * 100

      # F-test for equality of variances
      f_test <- tryCatch(
        var.test(post_vals, pre_vals),
        error = function(e) list(statistic = NA, p.value = NA)
      )

      results <- rbind(results, data.frame(
        Asset = col,
        Pre_Crisis_Vol = pre_vol,
        Crisis_Vol = post_vol,
        Change_Pct = vol_change_pct,
        F_Stat = f_test$statistic,
        P_Value = f_test$p.value,
        Significant = !is.na(f_test$p.value) && f_test$p.value < 0.05
      ))
    }
  }

  return(results)
}

# Function to test beta changes
test_beta_changes <- function(rolling_betas, crisis_start) {
  # Ensure required packages are available
  if (!requireNamespace("stats", quietly = TRUE)) {
    stop("Package 'stats' is needed. Please install it.", call. = FALSE)
  }
  
  # Convert crisis_start to Date
  crisis_date <- as.Date(crisis_start)
  
  # Split data
  pre_crisis <- rolling_betas[index(rolling_betas) < crisis_date]
  crisis <- rolling_betas[index(rolling_betas) >= crisis_date]
  
  # Initialize results with explicit structure
  results <- data.frame(
    Asset = character(0),
    T_Statistic = numeric(0),
    P_Value = numeric(0),
    Significant = logical(0),
    Mean_Difference = numeric(0),
    stringsAsFactors = FALSE
  )
  
  # Process each asset
  for (col in colnames(rolling_betas)) {
    # Extract pre and crisis betas
    pre_betas <- pre_crisis[, col]
    crisis_betas <- crisis[, col]
    
    # Skip if insufficient data
    if (length(pre_betas) < 2 || length(crisis_betas) < 2) {
      warning("Insufficient data for beta testing of ", col)
      next
    }
    
    # T-test with error handling
    t_result <- tryCatch({
      t.test(crisis_betas, pre_betas)
    }, error = function(e) {
      warning("Error in t-test for ", col, ": ", e$message)
      return(NULL)
    })
    
    # Skip if t-test failed
    if (is.null(t_result)) next
    
    # Create a single row
    asset_row <- data.frame(
      Asset = gsub("_beta$", "", col),
      T_Statistic = t_result$statistic,
      P_Value = t_result$p.value,
      Significant = t_result$p.value < 0.05,
      Mean_Difference = t_result$estimate[1] - t_result$estimate[2],
      stringsAsFactors = FALSE
    )
    
    # Add to results
    results <- rbind(results, asset_row)
  }
  
  return(results)
}

# Function to test event significance
test_event_significance <- function(event_returns, asset_cols, suffix = "_cum_return") {
  # Initialize results with explicit structure
  results <- data.frame(
    Asset = character(0),
    Mean_Event_Return = numeric(0),
    T_Statistic = numeric(0),
    P_Value = numeric(0),
    Significant = logical(0),
    stringsAsFactors = FALSE
  )
  
  # Process each asset
  for (asset in asset_cols) {
    col_name <- paste0(asset, suffix)
    
    # Skip if column doesn't exist
    if (!(col_name %in% colnames(event_returns))) {
      warning("Column ", col_name, " not found in event_returns")
      next
    }
    
    # Extract returns with error handling
    returns <- tryCatch({
      ret <- event_returns[[col_name]]
      ret[!is.na(ret)]
    }, error = function(e) {
      warning("Error extracting returns for ", asset, ": ", e$message)
      return(NULL)
    })
    
    # Skip if insufficient data
    if (is.null(returns) || length(returns) < 2) {
      warning("Insufficient data for event testing of ", asset)
      next
    }
    
    # One-sample t-test against zero with error handling
    t_result <- tryCatch({
      t.test(returns, mu = 0)
    }, error = function(e) {
      warning("Error in t-test for ", asset, ": ", e$message)
      return(NULL)
    })
    
    # Skip if t-test failed
    if (is.null(t_result)) next
    
    # Create a single row
    asset_row <- data.frame(
      Asset = asset,
      Mean_Event_Return = mean(returns, na.rm = TRUE),
      T_Statistic = t_result$statistic,
      P_Value = t_result$p.value,
      Significant = t_result$p.value < 0.05,
      stringsAsFactors = FALSE
    )
    
    # Add to results
    results <- rbind(results, asset_row)
  }
  
  return(results)
}

# Function to calculate rolling correlations
calculate_rolling_correlations <- function(returns, window_size = 60, market_index = NULL) {
  # Check if market_index is provided and exists in returns
  if (!is.null(market_index) && !market_index %in% colnames(returns)) {
    stop("Specified market index not found in returns data")
  }
  
  # Columns to calculate correlations for
  if (!is.null(market_index)) {
    # Calculate correlations between each column and the market index
    result <- matrix(NA, nrow = nrow(returns) - window_size + 1, 
                    ncol = ncol(returns) - 1)
    colnames(result) <- colnames(returns)[colnames(returns) != market_index]
    
    for (i in window_size:nrow(returns)) {
      window_data <- returns[(i-window_size+1):i, ]
      for (j in 1:ncol(result)) {
        col <- colnames(result)[j]
        result[i-window_size+1, j] <- cor(window_data[, col], 
                                         window_data[, market_index], 
                                         use = "pairwise.complete.obs")
      }
    }
  } else {
    # Calculate all pairwise correlations
    cols <- colnames(returns)
    pairs <- combn(cols, 2)
    result <- matrix(NA, nrow = nrow(returns) - window_size + 1, 
                    ncol = ncol(pairs))
    colnames(result) <- apply(pairs, 2, paste, collapse = "_")
    
    for (i in window_size:nrow(returns)) {
      window_data <- returns[(i-window_size+1):i, ]
      for (j in 1:ncol(pairs)) {
        pair <- pairs[, j]
        result[i-window_size+1, j] <- cor(window_data[, pair[1]], 
                                         window_data[, pair[2]], 
                                         use = "pairwise.complete.obs")
      }
    }
  }
  
  # Convert to xts
  roll_cor <- xts(result, order.by = index(returns)[window_size:nrow(returns)])
  return(roll_cor)
}

# Function to plot rolling correlations
plot_rolling_correlations <- function(rolling_cors, crisis_start, title = "Rolling Correlations") {
  # Convert to data frame for ggplot
  plot_data <- data.frame(
    Date = index(rolling_cors),
    as.data.frame(rolling_cors)
  )
  
  # Reshape to long format
  plot_data_long <- reshape2::melt(plot_data, id.vars = "Date",
                                  variable.name = "Pair",
                                  value.name = "Correlation")
  
  # Create plot
  p <- ggplot2::ggplot(plot_data_long, ggplot2::aes(x = Date, y = Correlation, color = Pair)) +
    ggplot2::geom_line() +
    ggplot2::geom_hline(yintercept = 0, linetype = "dotted", color = "darkgray") +
    ggplot2::geom_vline(xintercept = as.Date(crisis_start), 
               linetype = "dashed", color = "red") +
    ggplot2::labs(
      title = title,
      x = "Date",
      y = "Correlation Coefficient",
      color = "Asset Pair"
    ) +
    ggplot2::theme_minimal() +
    ggplot2::theme(legend.position = "bottom") +
    ggplot2::scale_color_brewer(palette = "Set1")
  
  return(p)
}

# Function to run GARCH analysis
run_garch_analysis <- function(return_series, asset_name) {
  # Ensure required packages are available
  if (!requireNamespace("rugarch", quietly = TRUE)) {
    stop("Package 'rugarch' is needed. Please install it.", call. = FALSE)
  }
  
  # Prepare data - convert xts to numeric vector
  returns_vector <- as.numeric(return_series)
  returns_vector <- returns_vector[!is.na(returns_vector)]
  
  # Specify GARCH model (standard GARCH(1,1) with normal distribution)
  garch_spec <- rugarch::ugarchspec(
    variance.model = list(model = "sGARCH", garchOrder = c(1, 1)),
    mean.model = list(armaOrder = c(1, 0), include.mean = TRUE),
    distribution.model = "norm"
  )
  
  # Fit the model
  garch_fit <- tryCatch({
    rugarch::ugarchfit(garch_spec, returns_vector)
  }, error = function(e) {
    warning("Error fitting GARCH model for ", asset_name, ": ", e$message)
    return(NULL)
  })
  
  # Return the fitted model
  return(list(
    model = garch_fit,
    asset = asset_name,
    spec = garch_spec
  ))
}

# Function to extract GARCH volatility
extract_garch_volatility <- function(garch_result, return_series) {
  # Extract conditional volatility from the GARCH model
  if (is.null(garch_result$model)) {
    return(NULL)
  }
  
  # Get sigma (conditional volatility)
  sigma <- rugarch::sigma(garch_result$model)
  
  # Create xts object aligned with return_series
  sigma_xts <- xts(sigma, order.by = index(return_series)[1:length(sigma)])
  
  return(sigma_xts)
}

# Function to plot GARCH volatility
plot_garch_volatility <- function(garch_volatilities, crisis_start, title = "GARCH Conditional Volatility") {
  # Combine all volatilities into one xts object
  combined <- do.call(merge, garch_volatilities)
  
  # Convert to data frame for ggplot
  plot_data <- data.frame(
    Date = index(combined),
    as.data.frame(combined)
  )
  
  # Reshape to long format
  plot_data_long <- reshape2::melt(plot_data, id.vars = "Date",
                                  variable.name = "Asset",
                                  value.name = "Volatility")
  
  # Create plot
  p <- ggplot2::ggplot(plot_data_long, ggplot2::aes(x = Date, y = Volatility, color = Asset)) +
    ggplot2::geom_line() +
    ggplot2::geom_vline(xintercept = as.Date(crisis_start), 
               linetype = "dashed", color = "red") +
    ggplot2::labs(
      title = title,
      x = "Date",
      y = "Conditional Volatility",
      color = "Asset"
    ) +
    ggplot2::theme_minimal() +
    ggplot2::theme(legend.position = "bottom") +
    ggplot2::scale_color_brewer(palette = "Set1")
  
  return(p)
}

# Function to run structural break tests
test_structural_breaks <- function(return_series, crisis_start) {
  # Ensure required packages are available
  if (!requireNamespace("strucchange", quietly = TRUE)) {
    stop("Package 'strucchange' is needed. Please install it.", call. = FALSE)
  }
  
  # Convert xts to ts
  returns_ts <- as.ts(return_series)
  
  # Run CUSUM test
  cusum_test <- tryCatch({
    strucchange::efp(returns_ts ~ 1, type = "OLS-CUSUM")
  }, error = function(e) {
    warning("Error in CUSUM test: ", e$message)
    return(NULL)
  })
  
  # Run Chow test at the crisis date
  # Find the closest index to the crisis date
  crisis_date <- as.Date(crisis_start)
  crisis_idx <- which.min(abs(as.numeric(index(return_series) - crisis_date)))
  
  chow_test <- tryCatch({
    strucchange::sctest(returns_ts ~ 1, type = "Chow", point = crisis_idx)
  }, error = function(e) {
    warning("Error in Chow test: ", e$message)
    return(NULL)
  })
  
  # Run Breakpoints test to identify optimal breaks
  bp_test <- tryCatch({
    strucchange::breakpoints(returns_ts ~ 1)
  }, error = function(e) {
    warning("Error in breakpoints test: ", e$message)
    return(NULL)
  })
  
  # Return results
  return(list(
    cusum = cusum_test,
    chow = chow_test,
    breakpoints = bp_test
  ))
}

# Function to analyze economic significance
analyze_economic_significance <- function(prices, returns, asset_cols, crisis_start, 
                                        trading_volume = NULL) {
  # Convert crisis_start to Date
  crisis_date <- as.Date(crisis_start)
  
  # Calculate market value changes
  value_changes <- data.frame(
    Asset = character(),
    Pre_Crisis_Price = numeric(),
    Post_Crisis_Price = numeric(),
    Price_Change_Pct = numeric(),
    Annualized_Return_Pre = numeric(),
    Annualized_Return_Post = numeric(),
    Return_Difference = numeric(),
    stringsAsFactors = FALSE
  )
  
  for (col in asset_cols) {
    if (col %in% colnames(prices)) {
      # Check if we have data before and after crisis date
      pre_crisis_data <- prices[index(prices) < crisis_date, col]
      post_crisis_data <- prices[index(prices) >= crisis_date, col]
      
      # Skip if insufficient data
      if (length(pre_crisis_data) == 0 || length(post_crisis_data) == 0) {
        warning("Insufficient price data for asset ", col, " before or after crisis date")
        next
      }
      
      # Extract pre/post prices for this asset safely
      pre_crisis_price_start <- as.numeric(prices[index(prices)[1], col])
      pre_crisis_price_end <- as.numeric(tail(pre_crisis_data, 1))
      post_crisis_price_latest <- as.numeric(tail(post_crisis_data, 1))
      
      # Skip if any price is NA
      if (any(is.na(c(pre_crisis_price_start, pre_crisis_price_end, post_crisis_price_latest)))) {
        warning("Missing price data for asset ", col)
        next
      }
      
      # Calculate price changes
      pre_crisis_change <- (pre_crisis_price_end / pre_crisis_price_start - 1) * 100
      post_crisis_change <- (post_crisis_price_latest / pre_crisis_price_end - 1) * 100
      
      # Calculate trading days in each period
      pre_days <- sum(index(prices) < crisis_date)
      post_days <- sum(index(prices) >= crisis_date)
      
      # Annualize returns (assuming 252 trading days per year)
      pre_annual <- (1 + pre_crisis_change/100)^(252/pre_days) - 1
      post_annual <- (1 + post_crisis_change/100)^(252/post_days) - 1
      
      # Add to results
      value_changes <- rbind(value_changes, data.frame(
        Asset = col,
        Pre_Crisis_Price = pre_crisis_price_end,
        Post_Crisis_Price = post_crisis_price_latest,
        Price_Change_Pct = post_crisis_change,
        Annualized_Return_Pre = pre_annual * 100,
        Annualized_Return_Post = post_annual * 100,
        Return_Difference = (post_annual - pre_annual) * 100,
        stringsAsFactors = FALSE
      ))
    }
  }
  
  # Calculate risk-adjusted returns (Sharpe ratios)
  risk_adjusted <- data.frame(
    Asset = character(),
    Pre_Crisis_Sharpe = numeric(),
    Crisis_Sharpe = numeric(),
    Sharpe_Change = numeric(),
    stringsAsFactors = FALSE
  )
  
  # Assume risk-free rate of 3%
  rf_daily <- 0.03 / 252
  
  for (col in asset_cols) {
    if (col %in% colnames(returns)) {
      # Extract returns for different periods
      pre_returns <- returns[index(returns) < crisis_date, col]
      crisis_returns <- returns[index(returns) >= crisis_date, col]
      
      # Skip if insufficient data
      if (length(pre_returns) < 5 || length(crisis_returns) < 5) {
        warning("Insufficient return data for asset ", col)
        next
      }
      
      # Calculate Sharpe ratios
      pre_mean <- mean(pre_returns, na.rm = TRUE) * 252 # Annualize
      pre_sd <- sd(pre_returns, na.rm = TRUE) * sqrt(252) # Annualize
      pre_sharpe <- (pre_mean - 0.03) / pre_sd
      
      crisis_mean <- mean(crisis_returns, na.rm = TRUE) * 252 # Annualize
      crisis_sd <- sd(crisis_returns, na.rm = TRUE) * sqrt(252) # Annualize
      crisis_sharpe <- (crisis_mean - 0.03) / crisis_sd
      
      # Add to results
      risk_adjusted <- rbind(risk_adjusted, data.frame(
        Asset = col,
        Pre_Crisis_Sharpe = pre_sharpe,
        Crisis_Sharpe = crisis_sharpe,
        Sharpe_Change = crisis_sharpe - pre_sharpe,
        stringsAsFactors = FALSE
      ))
    }
  }
  
  return(list(
    price_changes = value_changes,
    risk_adjusted = risk_adjusted
  ))
}
```

# Introduction

The Red Sea is one of the most important maritime trade routes in the world, with approximately 12% of global trade passing through this vital waterway. Since late 2023, Houthi rebel attacks on commercial vessels in response to the Israel-Hamas conflict have severely disrupted shipping operations in this region. Many shipping companies have been forced to reroute vessels around the Cape of Good Hope, adding significant time and costs to global supply chains.

This project aims to analyze the impact of the Red Sea crisis on global shipping and stock markets using time series methods. By comparing shipping-related financial instruments with broader market indices, we seek to quantify the specific effects of this geopolitical crisis on the shipping industry and related financial markets.

The Red Sea crisis represents a unique natural experiment to examine how geopolitical events affect specific industries differently from broader market movements. This analysis has implications for risk management, portfolio diversification, and understanding the economic impact of regional conflicts on global trade.

# Data Collection and Preprocessing

```{r data-collection}
# Define tickers and dates
shipping_tickers <- c("ZIM", "MAERSK-B.CO", "HLAG.DE", "DAC") # Shipping companies
index_tickers <- c("SPY", "XLE")                          # Market indices
oil_tickers <- c("USO")                                   # Oil ETF

start_date <- "2023-01-01" 
end_date <- "2025-04-30"
crisis_start <- "2023-11-19"  # Date of first Houthi attack

# Download data with error handling
shipping_data <- download_with_error_handling(shipping_tickers, start_date, end_date)
index_data <- download_with_error_handling(index_tickers, start_date, end_date)
oil_data <- download_with_error_handling(oil_tickers, start_date, end_date)

# Process data
data <- process_financial_data(shipping_data, index_data, oil_data, crisis_start)

# Display summary statistics
stats_table <- calculate_descriptive_stats(data$returns)
kable(stats_table, caption = "Summary Statistics of Daily Returns (%)",
      digits = 2, booktabs = TRUE)
```

# Descriptive Analysis

```{r descriptive-analysis}
# Compare pre-crisis and post-crisis periods
period_comparison <- compare_periods(data$returns, data$crisis_start)
formatted_kable(period_comparison, "Pre-Crisis vs Crisis Period Statistics", digits = 2)

# Plot price data with improved aesthetics
shipping_plot <- plot_prices(data$normalized_prices, names(shipping_data), 
                           data$crisis_start, "Shipping Assets Performance") +
  theme_report() +
  labs(subtitle = paste("Price Performance Before and After Red Sea Crisis (Start:", data$crisis_start, ")"))
print(shipping_plot)

market_plot <- plot_prices(data$normalized_prices, names(index_data), 
                         data$crisis_start, "Market Indices Performance") +
  theme_report() +
  labs(subtitle = "Comparison of Market Benchmarks During the Same Period")
print(market_plot)

# Calculate monthly statistics to show trends
monthly_stats <- data.frame()
for (col in data$shipping_cols) {
  if (col %in% colnames(data$returns)) {
    # Extract returns for this asset
    asset_returns <- data$returns[, col]
    
    # Skip if no data
    if (length(asset_returns) == 0 || all(is.na(asset_returns))) {
      warning(paste("No valid data for asset", col))
      next
    }
    
    # Create monthly statistics with error handling
    tryCatch({
      # Group by month
      months <- format(index(asset_returns), "%Y-%m")
      
      # Pre-allocate results data frame
      unique_months <- unique(months)
      month_df <- data.frame(
        Asset = rep(col, length(unique_months)),
        Month = unique_months,
        Mean = NA_real_,
        Volatility = NA_real_,
        Min = NA_real_,
        Max = NA_real_,
        stringsAsFactors = FALSE
      )
      
      # Calculate statistics for each month
      for (i in 1:length(unique_months)) {
        month_data <- asset_returns[months == unique_months[i]]
        if (length(month_data) >= 5) {  # Require at least 5 observations
          month_df$Mean[i] <- mean(month_data, na.rm = TRUE) * 100
          month_df$Volatility[i] <- sd(month_data, na.rm = TRUE) * 100
          month_df$Min[i] <- min(month_data, na.rm = TRUE) * 100
          month_df$Max[i] <- max(month_data, na.rm = TRUE) * 100
        }
      }
      
      # Remove rows with missing values
      month_df <- month_df[!is.na(month_df$Mean), ]
      
      # Flag crisis period
      if (nrow(month_df) > 0) {
        month_df$Period <- ifelse(as.Date(paste0(month_df$Month, "-01")) >= as.Date(data$crisis_start),
                                "Crisis", "Pre-Crisis")
        
        # Add to results
        monthly_stats <- rbind(monthly_stats, month_df)
      }
    }, error = function(e) {
      warning(paste("Error calculating monthly statistics for", col, ":", e$message))
    })
  }
}

# Display monthly statistics
if (nrow(monthly_stats) > 0) {
  formatted_kable(monthly_stats, "Monthly Return Statistics by Asset (%)", digits = 2)
  
  # Plot monthly volatility trends
  monthly_vol_plot <- ggplot(monthly_stats, aes(x = Month, y = Volatility, 
                                             color = Asset, group = Asset)) +
    geom_line(size = 1) +
    geom_point(size = 3) +
    geom_vline(xintercept = which(unique(monthly_stats$Month) == 
                              format(as.Date(data$crisis_start), "%Y-%m")),
               linetype = "dashed", color = "red") +
    labs(title = "Monthly Volatility Evolution",
         subtitle = "Standard Deviation of Daily Returns by Month (%)",
         x = "Month",
         y = "Volatility (%)") +
    theme_report() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  print(monthly_vol_plot)
}
```

# Stationarity and Correlation Analysis

```{r stationarity-correlation}
# Test stationarity
stationarity_results <- enhanced_stationarity_tests(data$returns)
formatted_kable(stationarity_results, "Stationarity Test Results", digits = 4)

# Create custom correlation heatmap function with improved aesthetics
improved_correlation_heatmap <- function(returns, period = "all", crisis_start = NULL, 
                                       title = "Correlation Heatmap") {
  # Filter data based on period if specified
  if (period != "all" && !is.null(crisis_start)) {
    crisis_date <- as.Date(crisis_start)
    if (period == "pre_crisis") {
      returns <- returns[index(returns) < crisis_date]
      title_suffix <- "Pre-Crisis Period"
    } else if (period == "crisis") {
      returns <- returns[index(returns) >= crisis_date]
      title_suffix <- "Crisis Period"
    }
  } else {
    title_suffix <- "Full Period"
  }
  
  # Calculate correlation matrix
  cor_matrix <- cor(returns, use = "pairwise.complete.obs")
  
  # Convert to long format for ggplot
  cor_data <- reshape2::melt(cor_matrix)
  
  # Create heatmap
  p <- ggplot(cor_data, aes(x = Var1, y = Var2, fill = value, label = round(value, 2))) +
    geom_tile(color = "white") +
    geom_text(size = 3, color = ifelse(abs(cor_data$value) > 0.5, "white", "black")) +
    scale_fill_gradient2(low = "darkblue", high = "darkred", mid = "white", 
                        midpoint = 0, limit = c(-1, 1), space = "Lab",
                        name = "Correlation") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
          axis.text.y = element_text(size = 9),
          axis.title = element_blank(),
          panel.grid = element_blank(),
          legend.position = "bottom") +
    coord_fixed() +
    labs(title = paste(title, "-", title_suffix))
  
  return(p)
}

# Create correlation heatmaps with improved aesthetics
all_corr <- improved_correlation_heatmap(data$returns, title = "Asset Correlation")
pre_crisis_corr <- improved_correlation_heatmap(data$returns, "pre_crisis", data$crisis_start, 
                                             title = "Asset Correlation")
crisis_corr <- improved_correlation_heatmap(data$returns, "crisis", data$crisis_start, 
                                         title = "Asset Correlation")

# Arrange correlation plots in a grid
grid.arrange(pre_crisis_corr, crisis_corr, ncol = 2)

# Analyze correlation changes
cor_change <- analyze_correlation_changes(data$returns, data$crisis_start)

# Create improved correlation change heatmap
cor_changes_long <- reshape2::melt(cor_change$changes)
change_heatmap <- ggplot(cor_changes_long, aes(x = Var1, y = Var2, fill = value, 
                                           label = round(value, 2))) +
  geom_tile(color = "white") +
  geom_text(size = 3, color = ifelse(abs(cor_changes_long$value) > 0.25, "white", "black")) +
  scale_fill_gradient2(low = "darkblue", high = "darkred", mid = "white", 
                      midpoint = 0, limit = c(-1, 1), space = "Lab",
                      name = "Correlation\nChange") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title = element_blank(),
        panel.grid = element_blank(),
        legend.position = "bottom") +
  coord_fixed() +
  labs(title = "Changes in Correlation During Crisis",
       subtitle = paste("Average Correlation Change:", 
                      round(mean(cor_change$changes[upper.tri(cor_change$changes)], na.rm = TRUE), 3)))

print(change_heatmap)

# Display summary statistics for correlation changes
formatted_kable(cor_change$summary, "Correlation Change Summary Statistics", digits = 4)

# Calculate and display the most significant correlation changes
# Pre-allocate a list to collect results
correlation_results <- list()
result_index <- 1

# Loop through unique asset pairs
for (i in 1:(ncol(data$returns)-1)) {
  for (j in (i+1):ncol(data$returns)) {
    # Get asset names
    asset1 <- colnames(data$returns)[i]
    asset2 <- colnames(data$returns)[j]
    
    # Skip processing if we don't have enough data
    tryCatch({
      # Extract data for different periods
      pre_crisis_data1 <- data$returns[index(data$returns) < as.Date(data$crisis_start), asset1]
      pre_crisis_data2 <- data$returns[index(data$returns) < as.Date(data$crisis_start), asset2]
      crisis_data1 <- data$returns[index(data$returns) >= as.Date(data$crisis_start), asset1]
      crisis_data2 <- data$returns[index(data$returns) >= as.Date(data$crisis_start), asset2]
      
      # Only proceed if we have enough data
      if (length(pre_crisis_data1) >= 10 && length(pre_crisis_data2) >= 10 && 
          length(crisis_data1) >= 10 && length(crisis_data2) >= 10) {
        
        # Calculate correlations for different periods
        pre_cor <- cor(pre_crisis_data1, pre_crisis_data2, use = "pairwise.complete.obs")
        crisis_cor <- cor(crisis_data1, crisis_data2, use = "pairwise.complete.obs")
        
        # Calculate change
        change <- crisis_cor - pre_cor
        pct_change <- ifelse(pre_cor != 0, (change / abs(pre_cor)) * 100, NA)
        
        # Store result in list (safer than row-by-row rbind)
        correlation_results[[result_index]] <- list(
          Asset_Pair = paste(asset1, "-", asset2),
          Pre_Crisis = pre_cor,
          Crisis = crisis_cor,
          Change = change,
          Pct_Change = pct_change
        )
        result_index <- result_index + 1
      }
    }, error = function(e) {
      # Just skip this pair if there's an error
      warning(paste("Error calculating correlation for", asset1, "-", asset2, ":", e$message))
    })
  }
}

# Convert list to data frame
if (length(correlation_results) > 0) {
  # Create data frame from list
  significant_changes <- data.frame(
    Asset_Pair = character(length(correlation_results)),
    Pre_Crisis = numeric(length(correlation_results)),
    Crisis = numeric(length(correlation_results)),
    Change = numeric(length(correlation_results)),
    Pct_Change = numeric(length(correlation_results)),
    stringsAsFactors = FALSE
  )
  
  # Fill data frame from list
  for (i in 1:length(correlation_results)) {
    significant_changes$Asset_Pair[i] <- correlation_results[[i]]$Asset_Pair
    significant_changes$Pre_Crisis[i] <- correlation_results[[i]]$Pre_Crisis
    significant_changes$Crisis[i] <- correlation_results[[i]]$Crisis
    significant_changes$Change[i] <- correlation_results[[i]]$Change
    significant_changes$Pct_Change[i] <- correlation_results[[i]]$Pct_Change
  }
  
  # Sort by absolute change
  significant_changes <- significant_changes[order(-abs(significant_changes$Change)),]
  
  # Display top changes
  if (nrow(significant_changes) > 0) {
    top_changes <- head(significant_changes, 5)
    formatted_kable(top_changes, "Top 5 Largest Correlation Changes", digits = 3)
  }
} else {
  cat("No significant correlation changes found between assets.\n")
}
```

# Event Analysis

```{r event-analysis, warning=FALSE}
# Define events with improved alignment to trading days
# First create function to find nearest trading day
find_nearest_trading_day <- function(event_date, returns_dates) {
  event_date <- as.Date(event_date)
  returns_dates <- as.Date(returns_dates)
  
  # Find closest trading day on or after the event
  future_days <- returns_dates[returns_dates >= event_date]
  if (length(future_days) > 0) {
    return(min(future_days))
  }
  
  # If no future trading days available, take the most recent past trading day
  past_days <- returns_dates[returns_dates < event_date]
  if (length(past_days) > 0) {
    return(max(past_days))
  }
  
  # If no trading days at all (should never happen), return original date
  return(event_date)
}

# Define major Red Sea crisis events
raw_events <- data.frame(
  Event = c("Initial Houthi Attacks", 
            "US Coalition Formed", 
            "Major Shipping Diversion",
            "Military Response",
            "Escalation of Attacks"),
  RawDate = as.Date(c("2023-11-19", 
                     "2023-12-18", 
                     "2024-01-05",
                     "2024-01-12",
                     "2024-02-19"))
)

# Align event dates with actual trading days
events <- raw_events
events$Date <- sapply(raw_events$RawDate, function(d) find_nearest_trading_day(d, index(data$returns)))
events$Original_Date <- raw_events$RawDate

# Display both original event dates and aligned trading dates
events_display <- events[, c("Event", "Original_Date", "Date")]
colnames(events_display) <- c("Event", "Original Date", "Aligned Trading Date")
formatted_kable(events_display, "Crisis Events with Trading Day Alignment")

# Calculate event returns with multiple window lengths
window_lengths <- c(5, 10, 20)  # 5-day, 10-day, and 20-day windows

# List to store results for different windows
all_event_returns <- list()

for (window in window_lengths) {
  # Calculate event returns
  event_returns <- calculate_event_returns(data$returns, events, window = window)
  all_event_returns[[as.character(window)]] <- event_returns
  
  # Display event returns
  if (!is.null(event_returns)) {
    kable(event_returns, caption = paste0(window, "-Day Cumulative Returns (%) After Key Events"),
          digits = 2, booktabs = TRUE)
  }
}

# Function for bootstrapped significance testing
bootstrap_event_significance <- function(returns, event_dates, assets, window_size = 20, 
                                       n_bootstrap = 1000) {
  # Initialize results dataframe
  results <- data.frame(
    Asset = character(),
    Event = character(),
    CAR = numeric(),
    Bootstrap_p_value = numeric(),
    Significant = logical(),
    stringsAsFactors = FALSE
  )
  
  # For each event and asset
  for (i in 1:nrow(event_dates)) {
    event_name <- event_dates$Event[i]
    event_date <- event_dates$Date[i]
    
    for (asset in assets) {
      if (asset %in% colnames(returns)) {
        # Find index of event date
        event_idx <- which(index(returns) == event_date)
        
        # Skip if event date not found or insufficient data after event
        if (length(event_idx) == 0 || event_idx + window_size > nrow(returns)) {
          next
        }
        
        # Calculate actual cumulative abnormal return
        actual_car <- sum(returns[(event_idx+1):(event_idx+window_size), asset], na.rm = TRUE) * 100
        
        # Bootstrapping
        bootstrap_cars <- numeric(n_bootstrap)
        for (b in 1:n_bootstrap) {
          # Sample random start dates (avoiding overlap with actual event window)
          eligible_starts <- setdiff(1:(nrow(returns) - window_size), 
                                   (event_idx - window_size):(event_idx + window_size))
          
          if (length(eligible_starts) == 0) {
            # Not enough data for bootstrapping
            bootstrap_cars[b] <- NA
            next
          }
          
          start_idx <- sample(eligible_starts, 1)
          bootstrap_cars[b] <- sum(returns[start_idx:(start_idx+window_size-1), asset], 
                                 na.rm = TRUE) * 100
        }
        
        # Calculate p-value (two-tailed test)
        bootstrap_p_value <- mean(abs(bootstrap_cars) >= abs(actual_car), na.rm = TRUE)
        
        # Add to results
        results <- rbind(results, data.frame(
          Asset = asset,
          Event = event_name,
          CAR = actual_car,
          Bootstrap_p_value = bootstrap_p_value,
          Significant = bootstrap_p_value < 0.05,
          stringsAsFactors = FALSE
        ))
      }
    }
  }
  
  return(results)
}

# Run bootstrapped significance tests
bootstrap_results <- bootstrap_event_significance(data$returns, events, data$shipping_cols)

# Display bootstrap results
if (nrow(bootstrap_results) > 0) {
  formatted_kable(bootstrap_results, "Bootstrapped Event Impact Significance Tests", digits = 4)
  
  # Create enhanced visualization of results
  significant_events <- bootstrap_results[bootstrap_results$Significant == TRUE, ]
  
  if (nrow(significant_events) > 0) {
    sig_plot <- ggplot(significant_events, 
                      aes(x = Event, y = CAR, fill = Asset)) +
      geom_bar(stat = "identity", position = "dodge") +
      geom_text(aes(label = sprintf("p=%.3f", Bootstrap_p_value)), 
               position = position_dodge(width = 0.9), vjust = -0.5, size = 3) +
      labs(title = "Statistically Significant Event Impacts",
           subtitle = "Based on bootstrapped significance testing",
           x = "Event", y = "Cumulative Abnormal Return (%)") +
      theme_report() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1))
    
    print(sig_plot)
  }
}

# Create consolidated plot for different window lengths
event_returns_plot_data <- data.frame()

# Process each window length
for (window in names(all_event_returns)) {
  er <- all_event_returns[[window]]
  if (is.null(er) || nrow(er) == 0) next
  
  # For each shipping asset
  for (asset in data$shipping_cols) {
    col_name <- paste0(asset, "_cum_return")
    if (!(col_name %in% colnames(er))) next
    
    # Extract data for this asset and window
    asset_data <- data.frame(
      Event = er$Event,
      Asset = asset,
      Window = paste0(window, "-Day"),
      Return = er[[col_name]]
    )
    
    event_returns_plot_data <- rbind(event_returns_plot_data, asset_data)
  }
}

# Create multi-window comparison plot if we have data
if (nrow(event_returns_plot_data) > 0) {
  # Plot
  window_comparison <- ggplot(event_returns_plot_data, 
                            aes(x = Event, y = Return, fill = Window)) +
    geom_bar(stat = "identity", position = "dodge") +
    facet_wrap(~ Asset, scales = "free_y") +
    labs(title = "Event Impact Across Multiple Time Windows",
         subtitle = "Comparing 5, 10, and 20-day cumulative returns",
         x = "", y = "Cumulative Return (%)") +
    theme_report() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1),
          legend.position = "top")
  
  print(window_comparison)
}
```

# Market Beta Analysis

```{r beta-analysis}
# Calculate rolling betas
if ("SPY_returns" %in% colnames(data$returns)) {
  rolling_betas <- calculate_rolling_beta(data$returns, "SPY_returns")
  
  # Plot betas
  beta_plot <- plot_rolling_betas(rolling_betas, data$crisis_start)
  print(beta_plot)
  
  # Analyze beta changes
  beta_changes <- analyze_beta_changes(rolling_betas, data$crisis_start)
  kable(beta_changes, caption = "Beta Changes Analysis",
        digits = 4, booktabs = TRUE)
}
```

# Value at Risk Analysis

```{r var-analysis}
# Calculate VaR with improved methodology
var_results <- compare_var_periods(data$returns, 
                                 c(data$shipping_cols, data$market_cols), 
                                 data$crisis_start)

# Display VaR results
formatted_kable(var_results, "Value at Risk Analysis", digits = 2)

# Plot VaR comparison with ECDF visualization
var_plot <- plot_var_comparison(var_results)
print(var_plot)

# Create ECDF plots for pre-crisis and crisis periods for first shipping stock
if (length(data$shipping_cols) > 0) {
  asset <- data$shipping_cols[1]
  pre_crisis_returns <- data$returns[index(data$returns) < as.Date(data$crisis_start), asset]
  crisis_returns <- data$returns[index(data$returns) >= as.Date(data$crisis_start), asset]
  
  # Calculate VaR at 95% confidence
  alpha <- 0.05
  
  # Pre-crisis ECDF
  pre_returns_sorted <- sort(as.numeric(pre_crisis_returns), decreasing = FALSE)
  pre_ecdf <- 1:length(pre_returns_sorted) / length(pre_returns_sorted)
  pre_var <- quantile(pre_returns_sorted, probs = alpha) * 100
  
  # Crisis ECDF
  crisis_returns_sorted <- sort(as.numeric(crisis_returns), decreasing = FALSE)
  crisis_ecdf <- 1:length(crisis_returns_sorted) / length(crisis_returns_sorted)
  crisis_var <- quantile(crisis_returns_sorted, probs = alpha) * 100
  
  # Create ECDF plots
  par(mfrow = c(1, 2))
  plot(x = pre_returns_sorted * 100, y = pre_ecdf, 
       xlab = "Returns (%)", ylab = "ECDF", 
       main = paste("Pre-Crisis ECDF for", asset), 
       type = "l", col = "blue")
  abline(v = pre_var, col = "red")
  text(x = pre_var, y = 0.5, labels = paste("VaR:", round(pre_var, 2), "%"), pos = 4)
  
  plot(x = crisis_returns_sorted * 100, y = crisis_ecdf, 
       xlab = "Returns (%)", ylab = "ECDF", 
       main = paste("Crisis ECDF for", asset), 
       type = "l", col = "darkred")
  abline(v = crisis_var, col = "red")
  text(x = crisis_var, y = 0.5, labels = paste("VaR:", round(crisis_var, 2), "%"), pos = 4)
  par(mfrow = c(1, 1))
}

# Analyze VaR changes
var_changes <- analyze_var_changes(var_results)
formatted_kable(var_changes$summary, "VaR Changes Analysis", digits = 2)
```

# Forecasting and Granger Causality Analysis

```{r forecasting, warning=FALSE}
# Helper function to validate forecasts through backtesting
validate_forecasts <- function(data, var_model, test_size = 10) {
  # This function creates out-of-sample forecasts by estimating models
  # on a training set and predicting the test set values
  
  # Ensure we have enough data
  n <- nrow(data)
  if (n <= test_size) {
    return(data.frame(
      Series = colnames(data),
      RMSE = NA, 
      MAE = NA, 
      MAPE = NA
    ))
  }
  
  # Split into training and test sets
  train_data <- data[1:(n-test_size),]
  test_data <- data[(n-test_size+1):n,]
  
  # Refit the VAR model on training data
  train_model <- tryCatch({
    vars::VAR(train_data, p = var_model$p, type = "const")
  }, error = function(e) {
    # If error occurs, fall back to p=1
    vars::VAR(train_data, p = 1, type = "const")
  })
  
  # Generate forecasts
  forecasts <- predict(train_model, n.ahead = test_size)
  
  # Calculate error metrics for each series
  error_metrics <- data.frame(
    Series = character(),
    RMSE = numeric(),
    MAE = numeric(),
    MAPE = numeric(),
    stringsAsFactors = FALSE
  )
  
  for (series in colnames(data)) {
    # Extract actual values
    actual <- as.numeric(test_data[, series])
    
    # Extract forecasted values
    predicted <- as.numeric(forecasts$fcst[[series]][, "fcst"])
    
    # Calculate error metrics
    rmse <- sqrt(mean((actual - predicted)^2, na.rm = TRUE))
    mae <- mean(abs(actual - predicted), na.rm = TRUE)
    
    # MAPE needs special handling for near-zero values
    abs_pct_errors <- abs((actual - predicted) / actual) * 100
    abs_pct_errors[is.infinite(abs_pct_errors) | is.nan(abs_pct_errors)] <- NA
    mape <- mean(abs_pct_errors, na.rm = TRUE)
    
    # Add to results
    error_metrics <- rbind(error_metrics, data.frame(
      Series = series,
      RMSE = rmse,
      MAE = mae,
      MAPE = mape,
      stringsAsFactors = FALSE
    ))
  }
  
  return(error_metrics)
}

# Function to format theme for report plots
theme_report <- function() {
  theme_minimal() +
    theme(
      plot.title = element_text(face = "bold", size = 14),
      plot.subtitle = element_text(size = 12, color = "gray30"),
      axis.title = element_text(size = 11),
      axis.text = element_text(size = 10),
      legend.title = element_text(size = 10),
      legend.text = element_text(size = 9),
      panel.grid.major = element_line(color = "gray90"),
      panel.grid.minor = element_line(color = "gray95")
    )
}

# Prepare VAR data
if (length(data$shipping_cols) > 0 && length(data$market_cols) > 0) {
  # Select columns for VAR - use more assets for richer analysis
  var_cols <- c(data$shipping_cols, data$market_cols[1])
  var_data <- data$returns[, var_cols]
  
  # Test each series for stationarity to confirm VAR appropriateness
  stationarity_results <- sapply(var_cols, function(col) {
    test_result <- adf.test(var_data[, col])
    c(ADF_Statistic = test_result$statistic, 
      P_Value = test_result$p.value,
      Stationary = test_result$p.value < 0.05)
  })
  formatted_kable(as.data.frame(t(stationarity_results)), 
                "Stationarity Tests for VAR Input Series", digits = 4)
  
  # Run enhanced VAR analysis with optimal lag selection
  var_results <- tryCatch({
    # Select optimal lags with multiple information criteria
    lag_selection <- vars::VARselect(var_data, lag.max = 10, type = "const")
    formatted_kable(as.data.frame(lag_selection$criteria), 
                  "VAR Lag Selection Criteria", digits = 4)
    
    # Use AIC as primary criterion for lag selection
    optimal_lag <- lag_selection$selection["AIC(n)"]
    
    # Fit VAR model with optimal lag
    vars::VAR(var_data, p = optimal_lag, type = "const")
  }, error = function(e) {
    # If error occurs, fall back to simpler model
    warning("Error in VAR estimation with optimal lags: ", e$message, 
           ". Falling back to p=1 model.")
    vars::VAR(var_data, p = 1, type = "const")
  })
  
  # Calculate and visualize Granger causality matrix
  granger_matrix <- matrix(NA, length(var_cols), length(var_cols))
  rownames(granger_matrix) <- var_cols
  colnames(granger_matrix) <- var_cols
  
  # Fill Granger causality matrix
  for (i in var_cols) {
    for (j in var_cols) {
      if (i != j) {
        # Test if column i Granger-causes column j
        test <- tryCatch({
          gc_test <- vars::causality(var_results, cause = i)$Granger
          gc_test$p.value < 0.05
        }, error = function(e) {
          NA
        })
        granger_matrix[i, j] <- test
      }
    }
  }
  
  # Display Granger causality results
  granger_df <- as.data.frame(granger_matrix)
  granger_df$From <- rownames(granger_df)
  granger_df_long <- reshape2::melt(granger_df, id.vars = "From", 
                                  variable.name = "To", 
                                  value.name = "Granger_Causes")
  granger_df_long <- granger_df_long[!is.na(granger_df_long$Granger_Causes), ]
  
  if(nrow(granger_df_long) > 0) {
    formatted_kable(granger_df_long, "Granger Causality Relationships (at 5% significance)")
    
    # Create heatmap of Granger causality
    granger_heatmap <- ggplot(granger_df_long, aes(x = From, y = To, fill = Granger_Causes)) +
      geom_tile(color = "white") +
      scale_fill_manual(values = c("FALSE" = "lightblue", "TRUE" = "darkred")) +
      labs(title = "Granger Causality Relationships",
           subtitle = "Dark red indicates significant causality at 5% level",
           x = "Causing Variable", y = "Affected Variable") +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1))
    
    print(granger_heatmap)
  }
  
  # Generate forecasts
  var_forecasts <- predict(var_results, n.ahead = 20)
  
  # Create improved forecast plots
  plots <- list()
  forecast_data <- data.frame()
  
  for (series in names(var_forecasts$fcst)) {
    # Extract forecast data for this series
    fcst_data <- var_forecasts$fcst[[series]]
    
    # Convert to data frame for ggplot
    df <- data.frame(
      h = 1:20,
      forecast = fcst_data[, 1],
      lower_ci = fcst_data[, 2],
      upper_ci = fcst_data[, 3],
      series = series
    )
    
    forecast_data <- rbind(forecast_data, df)
  }
  
  # Create multi-series forecast plot
  forecast_plot <- ggplot(forecast_data, aes(x = h, y = forecast, color = series)) +
    geom_line() +
    geom_ribbon(aes(ymin = lower_ci, ymax = upper_ci, fill = series), alpha = 0.2) +
    geom_hline(yintercept = 0, linetype = "dashed", color = "gray") +
    facet_wrap(~series, scales = "free_y") +
    labs(title = "VAR Forecasts for Returns",
         subtitle = "20-Day Horizon with 95% Confidence Intervals",
         x = "Horizon (Days)",
         y = "Forecasted Returns") +
    theme_report() +
    theme(legend.position = "none")
  
  print(forecast_plot)
  
  # Validate forecasts with back-testing
  error_metrics <- validate_forecasts(var_data, var_results)
  formatted_kable(error_metrics, "Forecast Error Metrics", digits = 4)
}
```

# Advanced ARIMA Forecasting

```{r arima-forecasting}
# Apply auto.arima forecasting to price series with improved diagnostics
cat("## ARIMA Forecasting with Enhanced Diagnostics ##\n\n")

# Create a list to store ARIMA forecasts
arima_forecasts <- list()
arima_models <- list()

# Function to run diagnostic tests
arima_diagnostics <- function(model, returns, asset_name) {
  # Extract residuals
  residuals <- residuals(model)
  
  # Set up multi-panel plot
  par(mfrow = c(2, 2))
  
  # Plot residuals
  plot(residuals, main = paste("Residuals -", asset_name),
       ylab = "Residuals", xlab = "Time")
  abline(h = 0, col = "red")
  
  # ACF of residuals
  acf(residuals, main = paste("ACF of Residuals -", asset_name))
  
  # PACF of residuals
  pacf(residuals, main = paste("PACF of Residuals -", asset_name))
  
  # QQ plot of residuals
  qqnorm(residuals, main = paste("Q-Q Plot -", asset_name))
  qqline(residuals, col = 2)
  
  # Reset plotting parameters
  par(mfrow = c(1, 1))
  
  # Run Ljung-Box test
  lb_test <- Box.test(residuals, lag = 20, type = "Ljung-Box")
  
  # Return diagnostic results
  return(list(
    ljung_box = lb_test,
    shapiro = shapiro.test(residuals),
    mean = mean(residuals),
    sd = sd(residuals)
  ))
}

# Pre-process returns to identify and handle missing values
clean_data <- function(series) {
  # Replace Inf and -Inf with NA
  series[is.infinite(series)] <- NA
  
  # Interpolate NAs if possible
  if (sum(is.na(series)) > 0 && sum(is.na(series)) < length(series)) {
    # Use linear interpolation for NAs
    zoo::na.approx(series, na.rm = FALSE)
  } else {
    series
  }
}

# Loop through shipping assets
for (asset in data$shipping_cols) {
  if (asset %in% colnames(data$prices)) {
    cat(paste0("\n------------------------------------\n"))
    cat(paste0("Processing ARIMA forecasting for ", asset, "...\n"))
    
    # Extract price series
    price_series <- data$prices[, asset]
    
    # Clean the data
    price_series <- clean_data(price_series)
    
    # Check for stationarity of the price series
    adf_test <- tryCatch({
      adf.test(price_series)
    }, error = function(e) {
      cat("Error in ADF test:", e$message, "\n")
      list(p.value = 1) # Default to assuming non-stationarity
    })
    
    cat("ADF test p-value:", round(adf_test$p.value, 4), "\n")
    cat("Series is", ifelse(adf_test$p.value < 0.05, "stationary", "non-stationary"), "\n")
    
    # Apply the appropriate transformation and modeling
    tryCatch({
      # Create a ts object for auto.arima
      model_data <- log(price_series)
      
      # Check for and handle NAs in log-transformed data
      if (any(is.na(model_data))) {
        cat("Warning: NAs found in log-transformed data. Attempting interpolation.\n")
        model_data <- clean_data(model_data)
        
        # If still have NAs, use original price series
        if (any(is.na(model_data))) {
          cat("Warning: Still have NAs after interpolation. Using price directly.\n")
          model_data <- price_series
        }
      }
      
      # Apply auto.arima with stepwise=TRUE as fallback if comprehensive search fails
      tryCatch({
        cat("Trying comprehensive auto.arima search...\n")
        arima_model <- auto.arima(model_data, ic = "aic", stepwise = FALSE, 
                                approximation = FALSE, seasonal = FALSE)
      }, error = function(e) {
        cat("Comprehensive search failed. Falling back to stepwise approach.\n")
        cat("Error:", e$message, "\n")
        
        tryCatch({
          arima_model <- auto.arima(model_data, ic = "aic", stepwise = TRUE, 
                                  approximation = TRUE, seasonal = FALSE)
        }, error = function(e) {
          cat("Stepwise approach failed too. Trying basic ARIMA(1,1,1).\n")
          arima_model <- Arima(model_data, order = c(1,1,1))
        })
      })
      
      # Store the model
      arima_models[[asset]] <- arima_model
      
      # Run diagnostic tests
      cat("\nRunning diagnostic tests...\n")
      diag_results <- arima_diagnostics(arima_model, model_data, asset)
      
      # Print diagnostic test results
      cat("\nLjung-Box test for autocorrelation in residuals:\n")
      cat("Chi-squared =", round(diag_results$ljung_box$statistic, 4), 
         "df =", diag_results$ljung_box$parameter,
         "p-value =", round(diag_results$ljung_box$p.value, 4), "\n")
      cat("Residuals are", ifelse(diag_results$ljung_box$p.value > 0.05, 
                               "uncorrelated (good)", 
                               "correlated (potential issue)"), "\n")
      
      cat("\nShapiro-Wilk test for normality of residuals:\n")
      cat("W =", round(diag_results$shapiro$statistic, 4), 
         "p-value =", round(diag_results$shapiro$p.value, 4), "\n")
      cat("Residuals are", ifelse(diag_results$shapiro$p.value > 0.05, 
                               "normally distributed (good)", 
                               "not normally distributed"), "\n")
      
      # Generate forecast
      forecast_horizon <- 30 # 30-day forecast
      arima_forecast <- forecast(arima_model, h = forecast_horizon, level = c(80, 95))
      arima_forecasts[[asset]] <- arima_forecast
      
      # Create data frames for actual and forecasted values
      actual_df <- data.frame(
        Date = index(price_series)[(nrow(price_series)-60):nrow(price_series)],
        Value = as.numeric(price_series[(nrow(price_series)-60):nrow(price_series)]),
        Type = "Actual"
      )
      
      # Generate future dates for forecast
      future_dates <- seq(from = index(price_series)[nrow(price_series)] + 1, 
                         by = "day", length.out = forecast_horizon)
      
      # Create forecast dataframe (backtransform if needed)
      forecast_df <- data.frame(
        Date = future_dates,
        Value = exp(as.numeric(arima_forecast$mean)),
        Lower80 = exp(as.numeric(arima_forecast$lower[,1])),
        Upper80 = exp(as.numeric(arima_forecast$upper[,1])),
        Lower95 = exp(as.numeric(arima_forecast$lower[,2])),
        Upper95 = exp(as.numeric(arima_forecast$upper[,2])),
        Type = "Forecast"
      )
      
      # Plot with ggplot2
      p <- ggplot() +
        # Plot actual data
        geom_line(data = actual_df, aes(x = Date, y = Value), color = "black") +
        # Plot forecasted mean
        geom_line(data = forecast_df, aes(x = Date, y = Value), color = "blue") +
        # Plot prediction intervals
        geom_ribbon(data = forecast_df, 
                   aes(x = Date, ymin = Lower95, ymax = Upper95), 
                   fill = "blue", alpha = 0.2) +
        geom_ribbon(data = forecast_df, 
                   aes(x = Date, ymin = Lower80, ymax = Upper80), 
                   fill = "blue", alpha = 0.3) +
        # Add vertical line at forecast start
        geom_vline(xintercept = as.numeric(index(price_series)[nrow(price_series)]), 
                 linetype = "dashed", color = "red") +
        # Add crisis start line if it falls within the plot range
        {
          if (as.Date(data$crisis_start) >= min(actual_df$Date) && 
              as.Date(data$crisis_start) <= max(forecast_df$Date)) {
            geom_vline(xintercept = as.Date(data$crisis_start), 
                      linetype = "dotted", color = "darkred")
          }
        } +
        # Labels and theme
        labs(title = paste("ARIMA Forecast for", asset),
             subtitle = paste("Model: ARIMA(", 
                            paste(arimaorder(arima_model), collapse=","), ")",
                            " with AIC =", round(arima_model$aic, 2)),
             x = "Date", y = "Price") +
        theme_report()
      
      print(p)
      
      # Evaluate model fit and forecast accuracy
      cat("\nModel Accuracy Metrics:\n")
      acc <- accuracy(arima_model)
      print(acc)
      
      # Run out-of-sample validation
      # Take last 30 days of data as validation set
      if (nrow(price_series) > 60) {
        train_data <- window(ts(log(price_series)), end = nrow(price_series) - 30)
        test_data <- window(ts(log(price_series)), start = nrow(price_series) - 29)
        
        train_model <- Arima(train_data, model = arima_model)
        test_forecast <- forecast(train_model, h = length(test_data))
        
        # Calculate out-of-sample accuracy
        cat("\nOut-of-Sample Forecast Accuracy:\n")
        out_acc <- accuracy(test_forecast, test_data)
        print(out_acc)
      }
      
    }, error = function(e) {
      cat("Error in ARIMA modeling for", asset, ":", e$message, "\n")
    })
  }
}

# Compare forecast accuracy across assets
if (length(arima_models) > 0) {
  accuracy_table <- data.frame(
    Asset = character(),
    Model = character(),
    AIC = numeric(),
    BIC = numeric(),
    In_Sample_RMSE = numeric(),
    Out_Sample_RMSE = numeric(),
    Ljung_Box_p = numeric(),
    Residuals_Normal = character(),
    stringsAsFactors = FALSE
  )
  
  for (asset in names(arima_models)) {
    tryCatch({
      model <- arima_models[[asset]]
      
      # Run diagnostic tests
      diag_results <- arima_diagnostics(model, log(data$prices[, asset]), asset)
      
      # Extract accuracy metrics
      acc <- accuracy(model)
      
      # Perform out-of-sample validation if possible
      out_sample_rmse <- NA
      if (nrow(data$prices[, asset]) > 60) {
        price_series <- data$prices[, asset]
        train_data <- window(ts(log(price_series)), end = nrow(price_series) - 30)
        test_data <- window(ts(log(price_series)), start = nrow(price_series) - 29)
        
        train_model <- Arima(train_data, model = model)
        test_forecast <- forecast(train_model, h = length(test_data))
        
        out_sample_acc <- accuracy(test_forecast, test_data)
        out_sample_rmse <- out_sample_acc[2, "RMSE"]  # Test RMSE
      }
      
      # Add to table
      accuracy_table <- rbind(accuracy_table, data.frame(
        Asset = asset,
        Model = paste0("ARIMA(", paste(arimaorder(model), collapse=","), ")"),
        AIC = model$aic,
        BIC = model$bic,
        In_Sample_RMSE = acc[1, "RMSE"],
        Out_Sample_RMSE = out_sample_rmse,
        Ljung_Box_p = diag_results$ljung_box$p.value,
        Residuals_Normal = ifelse(diag_results$shapiro$p.value > 0.05, "Yes", "No"),
        stringsAsFactors = FALSE
      ))
    }, error = function(e) {
      cat("Error analyzing forecast accuracy for", asset, ":", e$message, "\n")
    })
  }
  
  # Display accuracy comparison
  if (nrow(accuracy_table) > 0) {
    formatted_kable(accuracy_table, "ARIMA Model Comparison with Diagnostics", digits = 4)
  }
}
```

# Hypothesis Testing

```{r hypothesis-testing}
# Test volatility changes
vol_test <- test_volatility_changes(data$returns, data$shipping_cols, data$crisis_start)
formatted_kable(vol_test, "Volatility Change Tests", digits = 4)

# Test beta changes if available
if (exists("rolling_betas")) {
  beta_test <- test_beta_changes(rolling_betas, data$crisis_start)
  formatted_kable(beta_test, "Beta Change Tests", digits = 4)
}

# Test event significance
if (!is.null(event_returns)) {
  event_test <- test_event_significance(event_returns, data$shipping_cols)
  formatted_kable(event_test, "Event Impact Tests", digits = 4)
}

# Test VaR changes
if (exists("var_changes")) {
  var_test <- test_var_changes(var_changes)
  formatted_kable(var_test, "VaR Change Tests", digits = 4)
}
```

# Visual Summary of Key Findings

```{r visual-summary, fig.height=9, fig.width=10}
# Create a visual summary of key findings

# 1. Volatility comparison chart
if (nrow(vol_test) > 0) {
  vol_summary <- ggplot(vol_test, aes(x = Asset)) +
    geom_bar(aes(y = Pre_Crisis_Vol, fill = "Pre-Crisis"), stat = "identity", position = "dodge", width = 0.4) +
    geom_bar(aes(y = Crisis_Vol, fill = "Crisis"), stat = "identity", position = position_dodge(width = 0.4), width = 0.4) +
    geom_text(aes(y = Crisis_Vol + 0.2, label = paste0("+", round(Change_Pct, 0), "%")), 
             position = position_dodge(width = 0.4), vjust = 0, size = 3) +
    scale_fill_manual(values = c("Pre-Crisis" = "steelblue", "Crisis" = "darkred")) +
    labs(title = "Volatility Increase During Crisis",
         subtitle = "Standard Deviation of Daily Returns (%)",
         x = "",
         y = "Volatility (%)",
         fill = "Period") +
    theme_report() +
    theme(legend.position = "top")
  
  # 2. Risk metrics visual
  if (exists("var_changes") && is.list(var_changes) && "summary" %in% names(var_changes)) {
    # Create summary bar for percent of assets with increased risk
    risk_data <- data.frame(
      Metric = c("Assets with Increased Risk", "Assets with Decreased Risk"),
      Value = c(var_changes$summary$Pct_Assets_With_Increased_Risk, 
               100 - var_changes$summary$Pct_Assets_With_Increased_Risk),
      stringsAsFactors = FALSE
    )
    
    risk_summary <- ggplot(risk_data, aes(x = "", y = Value, fill = Metric)) +
      geom_bar(stat = "identity", width = 1) +
      geom_text(aes(label = paste0(round(Value, 0), "%")), 
               position = position_stack(vjust = 0.5), color = "white") +
      coord_polar("y", start = 0) +
      scale_fill_manual(values = c("Assets with Increased Risk" = "darkred", 
                                 "Assets with Decreased Risk" = "darkgreen")) +
      labs(title = "Risk Profile Changes",
           subtitle = "Proportion of Assets with VaR Changes",
           x = NULL, y = NULL, fill = NULL) +
      theme_minimal() +
      theme(axis.text = element_blank(),
            axis.ticks = element_blank(),
            panel.grid = element_blank())
    
    # Arrange the two plots
    grid.arrange(vol_summary, risk_summary, ncol = 2)
  } else {
    print(vol_summary)
  }
}

# 3. Timeline of key events
events <- data.frame(
  Date = as.Date(c("2023-11-19", "2023-12-18", "2024-01-12", "2024-01-16", "2024-02-14")),
  Event = c("First Houthi Attack", "Shipping Diversions", "Military Response", 
           "Maersk Suspension", "Bulk Carrier Attack"),
  Impact = c(1, 2, 3, 2, 1) # Relative impact scale (1-3)
)

# Create timeline visualization
timeline_plot <- ggplot(events, aes(x = Date, y = 1, size = Impact, color = factor(Impact))) +
  geom_point() +
  geom_segment(aes(x = min(events$Date) - 10, xend = max(events$Date) + 10, 
                  y = 1, yend = 1), color = "gray50", size = 0.5) +
  geom_text(aes(label = Event), vjust = -1.5, hjust = 0.5, size = 3) +
  geom_text(aes(label = format(Date, "%b %d")), vjust = 2, hjust = 0.5, size = 3) +
  scale_size_continuous(range = c(4, 10)) +
  scale_color_manual(values = c("1" = "steelblue", "2" = "darkred", "3" = "purple")) +
  labs(title = "Key Red Sea Crisis Events",
       subtitle = "Timeline of Major Developments",
       x = "", y = "") +
  theme_minimal() +
  theme(axis.text.y = element_blank(),
        axis.ticks = element_blank(),
        panel.grid = element_blank(),
        legend.position = "none")

print(timeline_plot)

# 4. Economic impact summary (if available)
if (exists("econ_results") && is.list(econ_results) && "price_changes" %in% names(econ_results)) {
  # Prepare data for economic impact visualization
  shipping_only <- econ_results$price_changes[econ_results$price_changes$Asset %in% data$shipping_cols, ]
  market_only <- econ_results$price_changes[econ_results$price_changes$Asset %in% data$market_cols, ]
  
  if (nrow(shipping_only) > 0 && nrow(market_only) > 0) {
    avg_shipping <- mean(shipping_only$Price_Change_Pct, na.rm = TRUE)
    avg_market <- mean(market_only$Price_Change_Pct, na.rm = TRUE)
    
    impact_data <- data.frame(
      Sector = c("Shipping Stocks", "Market Indices"),
      Change = c(avg_shipping, avg_market),
      stringsAsFactors = FALSE
    )
    
    econ_chart <- ggplot(impact_data, aes(x = Sector, y = Change, fill = Sector)) +
      geom_bar(stat = "identity", width = 0.6) +
      geom_text(aes(label = paste0(round(Change, 1), "%"), 
                   y = ifelse(Change > 0, Change + 1, Change - 1)),
               size = 4) +
      scale_fill_manual(values = c("Shipping Stocks" = "darkred", "Market Indices" = "steelblue")) +
      labs(title = "Economic Impact Comparison",
           subtitle = "Average Price Change (%)",
           x = "", y = "Percentage Change (%)") +
      theme_report() +
      theme(legend.position = "none")
    
    print(econ_chart)
  }
}
```

# Enhanced Analysis

## Rolling Correlations Analysis

```{r rolling-correlations}
# Calculate rolling correlations between shipping and market
if ("SPY" %in% colnames(data$returns)) {
  # Calculate rolling correlations with market
  rolling_cors <- calculate_rolling_correlations(data$returns, window_size = 60, market_index = "SPY")
  
  # Plot rolling correlations
  rolling_cors_plot <- plot_rolling_correlations(rolling_cors, data$crisis_start, 
                                               "Rolling 60-Day Correlations with S&P 500") +
    theme_report()
  print(rolling_cors_plot)
  
  # Calculate shipping correlations among themselves
  if (length(data$shipping_cols) > 1) {
    shipping_cors <- calculate_rolling_correlations(data$returns[, data$shipping_cols], window_size = 60)
    shipping_cors_plot <- plot_rolling_correlations(shipping_cors, data$crisis_start,
                                                  "Rolling 60-Day Correlations Between Shipping Stocks") +
      theme_report()
    print(shipping_cors_plot)
  }
}
```

## GARCH Volatility Modeling

```{r garch-analysis}
# Load required packages with enhanced error handling
required_packages <- c("rugarch", "zoo", "xts")
for(pkg in required_packages) {
  if(!requireNamespace(pkg, quietly = TRUE)) {
    cat(paste0("Package '", pkg, "' is required but not available. Some analyses may be limited.\n"))
  }
}

if (requireNamespace("rugarch", quietly = TRUE)) {
  # Load the library
  library(rugarch)
  
  # Run enhanced GARCH analysis on shipping stocks
  garch_models <- list()
  garch_vols <- list()
  ewma_vols <- list() # For EWMA volatility as fallback
  
  # EWMA volatility function as fallback
  calculate_ewma_volatility <- function(returns, lambda = 0.94) {
    # Initialize variance vector
    n <- length(returns)
    variance <- numeric(n)
    
    # Set initial variance to sample variance of first 20 observations or all if less
    init_window <- min(20, n)
    variance[1] <- ifelse(init_window > 1, var(returns[1:init_window], na.rm = TRUE), returns[1]^2)
    
    # Calculate EWMA variance
    for (i in 2:n) {
      if (is.na(returns[i-1])) {
        variance[i] <- variance[i-1]  # If return is NA, use previous variance
      } else {
        variance[i] <- lambda * variance[i-1] + (1 - lambda) * returns[i-1]^2
      }
    }
    
    # Convert to volatility (standard deviation)
    volatility <- sqrt(variance)
    
    # Return as xts object
    return(volatility)
  }
  
  # Summary table for GARCH results
  garch_summary <- data.frame(
    Asset = character(),
    Model = character(),
    Method = character(), # GARCH or EWMA fallback
    Mean_Equation = character(),
    Omega = numeric(),
    Alpha = numeric(),
    Beta = numeric(),
    Persistence = numeric(),
    AIC = numeric(),
    Convergence = character(),
    Pre_Crisis_Vol = numeric(),
    Crisis_Vol = numeric(),
    Vol_Change_Pct = numeric(),
    stringsAsFactors = FALSE
  )
  
  # Analyze all shipping stocks and main market index
  assets_to_analyze <- c(data$shipping_cols, data$market_cols[1]) 
  
  for (asset in assets_to_analyze) {
    if (asset %in% colnames(data$returns)) {
      cat("\n---------------------------------------------\n")
      cat("GARCH Analysis for", asset, "\n")
      cat("---------------------------------------------\n")
      
      # Extract return series and clean it
      returns_series <- as.numeric(data$returns[, asset])
      
      # Handle missing values
      if (sum(is.na(returns_series)) > 0) {
        cat("Warning:", sum(is.na(returns_series)), "missing values found. Interpolating...\n")
        # Interpolate missing values if possible
        if (sum(is.na(returns_series)) < length(returns_series) / 2) {
          returns_series <- zoo::na.approx(returns_series, na.rm = FALSE)
          returns_series[is.na(returns_series)] <- mean(returns_series, na.rm = TRUE)
        }
      }
      
      # Remove any remaining NAs
      returns_vector <- returns_series[!is.na(returns_series)]
      
      if (length(returns_vector) >= 100) { # Need sufficient observations
        # Try standardized GARCH(1,1) model first - the most robust specification
        cat("Attempting to fit standard GARCH(1,1) model...\n")
        
        model_converged <- FALSE
        use_ewma_fallback <- FALSE
        
        # Try GARCH(1,1) with normal distribution
        tryCatch({
          garch_spec <- rugarch::ugarchspec(
            variance.model = list(model = "sGARCH", garchOrder = c(1, 1)),
            mean.model = list(armaOrder = c(0, 0), include.mean = TRUE),
            distribution.model = "norm"
          )
          
          garch_fit <- rugarch::ugarchfit(garch_spec, returns_vector)
          
          # Check convergence
          if (garch_fit@fit$convergence == 0) {
            cat("GARCH(1,1) model converged successfully.\n")
            model_converged <- TRUE
            garch_result <- list(model = garch_fit, asset = asset, spec = garch_spec)
          } else {
            cat("GARCH(1,1) model did not converge. Trying alternatives...\n")
          }
        }, error = function(e) {
          cat("Error fitting GARCH(1,1) model:", e$message, "\n")
        })
        
        # If standard GARCH(1,1) failed, try with student-t distribution
        if (!model_converged) {
          cat("Trying GARCH(1,1) with Student-t distribution...\n")
          tryCatch({
            garch_spec <- rugarch::ugarchspec(
              variance.model = list(model = "sGARCH", garchOrder = c(1, 1)),
              mean.model = list(armaOrder = c(0, 0), include.mean = TRUE),
              distribution.model = "std"
            )
            
            garch_fit <- rugarch::ugarchfit(garch_spec, returns_vector)
            
            # Check convergence
            if (garch_fit@fit$convergence == 0) {
              cat("GARCH(1,1) with Student-t distribution converged.\n")
              model_converged <- TRUE
              garch_result <- list(model = garch_fit, asset = asset, spec = garch_spec)
            } else {
              cat("GARCH(1,1) with Student-t distribution did not converge.\n")
            }
          }, error = function(e) {
            cat("Error fitting GARCH(1,1) with Student-t:", e$message, "\n")
          })
        }
        
        # If still not converged, try GJR-GARCH (handles asymmetry better)
        if (!model_converged) {
          cat("Trying GJR-GARCH model...\n")
          tryCatch({
            garch_spec <- rugarch::ugarchspec(
              variance.model = list(model = "gjrGARCH", garchOrder = c(1, 1)),
              mean.model = list(armaOrder = c(0, 0), include.mean = TRUE),
              distribution.model = "norm"
            )
            
            garch_fit <- rugarch::ugarchfit(garch_spec, returns_vector)
            
            # Check convergence
            if (garch_fit@fit$convergence == 0) {
              cat("GJR-GARCH model converged.\n")
              model_converged <- TRUE
              garch_result <- list(model = garch_fit, asset = asset, spec = garch_spec)
            } else {
              cat("GJR-GARCH model did not converge.\n")
            }
          }, error = function(e) {
            cat("Error fitting GJR-GARCH model:", e$message, "\n")
          })
        }
        
        # If still not converged, try simplified ARCH(1) model
        if (!model_converged) {
          cat("Trying simplified ARCH(1) model...\n")
          tryCatch({
            garch_spec <- rugarch::ugarchspec(
              variance.model = list(model = "sGARCH", garchOrder = c(1, 0)),
              mean.model = list(armaOrder = c(0, 0), include.mean = TRUE),
              distribution.model = "norm"
            )
            
            garch_fit <- rugarch::ugarchfit(garch_spec, returns_vector)
            
            # Check convergence
            if (garch_fit@fit$convergence == 0) {
              cat("ARCH(1) model converged.\n")
              model_converged <- TRUE
              garch_result <- list(model = garch_fit, asset = asset, spec = garch_spec)
            } else {
              cat("ARCH(1) model did not converge. Falling back to EWMA volatility.\n")
              use_ewma_fallback <- TRUE
            }
          }, error = function(e) {
            cat("Error fitting ARCH(1) model:", e$message, "\n")
            use_ewma_fallback <- TRUE
          })
        }
        
        # If all GARCH models failed, use EWMA volatility as fallback
        if (use_ewma_fallback || !model_converged) {
          cat("Using EWMA volatility as fallback method...\n")
          
          # Calculate EWMA volatility
          returns_xts <- xts(returns_series, order.by = index(data$returns))
          ewma_vol <- calculate_ewma_volatility(returns_series)
          ewma_vol_xts <- xts(ewma_vol, order.by = index(data$returns))
          
          # Store in ewma_vols list
          ewma_vols[[asset]] <- ewma_vol_xts
          
          # Add to summary table
          pre_crisis <- ewma_vol_xts[index(ewma_vol_xts) < as.Date(data$crisis_start)]
          crisis <- ewma_vol_xts[index(ewma_vol_xts) >= as.Date(data$crisis_start)]
          
          pre_mean <- mean(pre_crisis, na.rm = TRUE)
          crisis_mean <- mean(crisis, na.rm = TRUE)
          pct_change <- (crisis_mean / pre_mean - 1) * 100
          
          garch_summary <- rbind(garch_summary, data.frame(
            Asset = asset,
            Model = "EWMA",
            Method = "EWMA Fallback",
            Mean_Equation = NA,
            Omega = NA,
            Alpha = 0.06, # For EWMA with lambda=0.94
            Beta = 0.94,  # Lambda
            Persistence = 1.00,
            AIC = NA,
            Convergence = "N/A (EWMA)",
            Pre_Crisis_Vol = pre_mean,
            Crisis_Vol = crisis_mean,
            Vol_Change_Pct = pct_change,
            stringsAsFactors = FALSE
          ))
          
        } else {
          # If GARCH model converged, extract and store results
          garch_models[[asset]] <- garch_result
          
          # Extract conditional volatility from the GARCH model
          sigma <- rugarch::sigma(garch_result$model)
          
          # Create xts object aligned with return_series
          sigma_xts <- xts(sigma, order.by = index(data$returns)[1:length(sigma)])
          garch_vols[[asset]] <- sigma_xts
          
          # Extract model parameters
          model_coef <- coef(garch_result$model)
          
          # Get model properties
          if (grepl("gjr", garch_result$spec@model$modeldesc$vmodel)) {
            model_type <- "GJR-GARCH"
          } else if (garch_result$spec@model$modelinc[["garchOrder"]][2] == 0) {
            model_type <- "ARCH"
          } else {
            model_type <- "GARCH"
          }
          
          garch_order <- garch_result$spec@model$modelinc[["garchOrder"]]
          arma_order <- garch_result$spec@model$modelinc[["armaOrder"]]
          dist_model <- garch_result$spec@model$modeldesc$distribution
          
          # Calculate persistence
          alpha_indices <- grep("alpha", names(model_coef))
          beta_indices <- grep("beta", names(model_coef))
          
          if (length(alpha_indices) > 0 && length(beta_indices) > 0) {
            persistence <- sum(model_coef[alpha_indices]) + sum(model_coef[beta_indices])
            
            # Compare pre-crisis and crisis period volatility
            pre_crisis <- sigma_xts[index(sigma_xts) < as.Date(data$crisis_start)]
            crisis <- sigma_xts[index(sigma_xts) >= as.Date(data$crisis_start)]
            
            pre_mean <- mean(pre_crisis, na.rm = TRUE)
            crisis_mean <- mean(crisis, na.rm = TRUE)
            pct_change <- (crisis_mean / pre_mean - 1) * 100
            
            # Add to summary
            garch_summary <- rbind(garch_summary, data.frame(
              Asset = asset,
              Model = paste0(model_type, "(", garch_order[1], ",", garch_order[2], ")"),
              Method = "GARCH",
              Mean_Equation = paste0("ARMA(", arma_order[1], ",", arma_order[2], ")"),
              Omega = model_coef["omega"],
              Alpha = sum(model_coef[alpha_indices]),
              Beta = sum(model_coef[beta_indices]),
              Persistence = persistence,
              AIC = garch_result$model@fit$ics["Akaike"],
              Convergence = "Converged",
              Pre_Crisis_Vol = pre_mean,
              Crisis_Vol = crisis_mean,
              Vol_Change_Pct = pct_change,
              stringsAsFactors = FALSE
            ))
            
            # Plot model diagnostics
            plot(garch_result$model, which = "all")
          }
        }
      } else {
        cat("Insufficient data for GARCH modeling of", asset, "\n")
      }
    }
  }
  
  # Display GARCH/EWMA summary table
  if (nrow(garch_summary) > 0) {
    formatted_kable(garch_summary, "Volatility Model Summary", digits = 4)
  }
  
  # Combine all volatilities (both GARCH and EWMA) for plotting
  all_vols <- list()
  for (asset in names(garch_vols)) {
    all_vols[[asset]] <- garch_vols[[asset]]
  }
  for (asset in names(ewma_vols)) {
    if (!(asset %in% names(all_vols))) {
      all_vols[[asset]] <- ewma_vols[[asset]]
    }
  }
  
  if (length(all_vols) > 0) {
    # Combine all volatilities
    combined_vols <- do.call(merge, all_vols)
    
    # Convert to data frame for ggplot
    plot_data <- data.frame(
      Date = index(combined_vols),
      as.data.frame(combined_vols)
    )
    
    # Reshape to long format
    plot_data_long <- reshape2::melt(plot_data, id.vars = "Date",
                                   variable.name = "Asset",
                                   value.name = "Volatility")
    
    # Create enhanced volatility plot
    vol_plot <- ggplot(plot_data_long, aes(x = Date, y = Volatility * 100, color = Asset)) +
      geom_line(linewidth = 0.8) +
      geom_vline(xintercept = as.Date(data$crisis_start), 
                 linetype = "dashed", color = "darkred", linewidth = 1) +
      labs(title = "Conditional Volatility Dynamics",
           subtitle = "Daily volatility estimated via GARCH or EWMA (with fallback)",
           x = "Date",
           y = "Annualized Volatility (%)",
           caption = "Red vertical line marks the start of the Red Sea crisis") +
      theme_report() +
      theme(legend.position = "bottom") +
      scale_y_continuous(labels = function(x) sprintf("%.1f%%", x)) +
      annotate("text", x = as.Date(data$crisis_start) + 15, 
              y = max(plot_data_long$Volatility, na.rm = TRUE) * 100 * 0.9, 
              label = "Crisis Start", color = "darkred", hjust = 0)
    
    print(vol_plot)
    
    # Create crisis vs pre-crisis volatility comparison
    vol_comparison <- garch_summary[, c("Asset", "Model", "Pre_Crisis_Vol", "Crisis_Vol", "Vol_Change_Pct")]
    vol_comparison$Pre_Crisis_Vol <- vol_comparison$Pre_Crisis_Vol * 100
    vol_comparison$Crisis_Vol <- vol_comparison$Crisis_Vol * 100
    
    formatted_kable(vol_comparison, "Volatility Comparison: Pre-Crisis vs Crisis", digits = 2)
    
    # Plot volatility comparison
    vol_comp_plot <- ggplot(vol_comparison, aes(x = Asset, fill = Asset)) +
      geom_bar(aes(y = Pre_Crisis_Vol), stat = "identity", position = position_dodge(), 
              alpha = 0.6, width = 0.4) +
      geom_bar(aes(y = Crisis_Vol), stat = "identity", position = position_dodge(width = 0.4), 
              alpha = 0.9, width = 0.4) +
      geom_text(aes(y = Crisis_Vol + 1, 
                  label = sprintf("+%.1f%%", Vol_Change_Pct)), 
               position = position_dodge(width = 0.4), size = 3, vjust = 0) +
      labs(title = "Volatility Increase During Crisis",
           subtitle = "Comparison of pre-crisis and crisis period volatility",
           x = "",
           y = "Daily Volatility (%)") +
      theme_report() +
      theme(legend.position = "none") +
      scale_y_continuous(labels = function(x) sprintf("%.1f%%", x))
    
    print(vol_comp_plot)
    
    # Conduct statistical test for volatility change
    vol_test_results <- data.frame(
      Asset = character(),
      F_Statistic = numeric(),
      P_Value = numeric(),
      Significant = logical(),
      stringsAsFactors = FALSE
    )
    
    for (asset in assets_to_analyze) {
      if (asset %in% names(all_vols)) {
        vols <- all_vols[[asset]]
        
        # Split by crisis date
        pre_crisis_vols <- as.numeric(vols[index(vols) < as.Date(data$crisis_start)])
        crisis_vols <- as.numeric(vols[index(vols) >= as.Date(data$crisis_start)])
        
        # Remove NAs
        pre_crisis_vols <- pre_crisis_vols[!is.na(pre_crisis_vols)]
        crisis_vols <- crisis_vols[!is.na(crisis_vols)]
        
        if (length(pre_crisis_vols) >= 30 && length(crisis_vols) >= 30) {
          # Test if variances are equal using F-test
          f_test <- var.test(crisis_vols, pre_crisis_vols)
          
          vol_test_results <- rbind(vol_test_results, data.frame(
            Asset = asset,
            F_Statistic = f_test$statistic,
            P_Value = f_test$p.value,
            Significant = f_test$p.value < 0.05,
            stringsAsFactors = FALSE
          ))
        }
      }
    }
    
    if (nrow(vol_test_results) > 0) {
      formatted_kable(vol_test_results, "Statistical Test for Volatility Change", digits = 4)
    }
  } else {
    cat("Package 'rugarch' is not available. GARCH analysis will be skipped.\n")
    cat("Consider installing rugarch with: install.packages('rugarch')\n")
  }
}
```

## Structural Break Testing

```{r structural-breaks}
# Run structural break tests on shipping stocks
structural_break_tests <- list()

for (asset in data$shipping_cols) {
  if (asset %in% colnames(data$returns)) {
    cat("\n---------------------------------------------\n")
    cat("Structural Break Testing for", asset, "\n")
    cat("---------------------------------------------\n")
    
    # Extract return series
    returns_series <- as.numeric(data$returns[, asset])
    
    # Run structural break tests
    test_results <- test_structural_breaks(returns_series, data$crisis_start)
    
    # Store results
    structural_break_tests[[asset]] <- test_results
    
    # Print test results
    cat("\nCUSUM Test:\n")
    print(test_results$cusum)
    cat("\nChow Test:\n")
    print(test_results$chow)
    cat("\nBreakpoints Test:\n")
    print(test_results$breakpoints)
  }
}

# Display structural break test results
if (length(structural_break_tests) > 0) {
  for (asset in names(structural_break_tests)) {
    cat("\nStructural Break Test Results for", asset, "\n")
    cat("---------------------------------------------\n")
    print(structural_break_tests[[asset]])
  }
}
```

# Conclusion

# Future Work and Research Extensions

This analysis provides important insights into how the Red Sea crisis affected shipping stocks and market relationships. However, several extensions would strengthen the findings and provide more comprehensive understanding:

## Incorporate Real Shipping Cost Data

```{r shipping-rate-discussion, eval=FALSE, echo=FALSE}
# This code is not evaluated but shows how shipping rate data could be incorporated
# Download Baltic Dry Index data
baltic_dry_index <- getSymbols("BDI", src="FRED", auto.assign=FALSE)

# Download Drewry World Container Index data (would need subscription)
# drewry_index <- read.csv("path/to/drewry_data.csv")

# Calculate percentage change pre vs post crisis
# pre_crisis_rates <- mean(shipping_index[index(shipping_index) < as.Date(data$crisis_start)])
# crisis_rates <- mean(shipping_index[index(shipping_index) >= as.Date(data$crisis_start)])
# rate_change_pct <- (crisis_rates / pre_crisis_rates - 1) * 100
```

The current economic impact assessment relies primarily on stock price changes as a proxy for shipping costs. Future work should:

-   Incorporate actual shipping rate indices like the Drewry World Container Index, Baltic Dry Index, and Shanghai Containerized Freight Index
-   Calculate direct economic impact based on actual freight rate increases rather than inferring from stock prices
-   Compare route-specific cost increases (e.g., Asia-Europe vs. Asia-Americas routes)
-   Analyze shipping volumes to understand both price and volume effects

## Improve Event Study Methodology

```{r event-study-extension, eval=FALSE, echo=FALSE}
# Example of bootstrapped significance testing
# bootstrap_significance <- function(returns, event_date, window_lengths = c(5, 10, 20)) {
#   results <- list()
#   for (window in window_lengths) {
#     # Actual returns
#     actual_car <- sum(returns[(event_date+1):(event_date+window)])
#     
#     # Bootstrap distribution
#     bootstrap_cars <- numeric(1000)
#     for (i in 1:1000) {
#       random_start <- sample(1:(length(returns)-window), 1)
#       bootstrap_cars[i] <- sum(returns[random_start:(random_start+window-1)])
#     }
#     
#     # Calculate p-value
#     p_value <- mean(abs(bootstrap_cars) >= abs(actual_car))
#     
#     results[[paste0("window_",window)]] <- list(
#       CAR = actual_car,
#       p_value = p_value,
#       significant = p_value < 0.05
#     )
#   }
#   return(results)
# }
```

The current event study could be enhanced by:

-   Implementing bootstrapped significance testing to address small sample size issues
-   Extending analysis to include multiple event windows (5-day, 10-day, and 20-day) for robustness
-   Applying market model adjustments to isolate abnormal returns more precisely
-   Conducting cross-sectional tests to identify determinants of heterogeneous responses

## Broaden Analysis Scope

```{r expanded-sectors, eval=FALSE, echo=FALSE}
# Example of related sectors to include
# logistics_tickers <- c("CHRW", "EXPD", "JBHT") # Logistics firms
# insurance_tickers <- c("BRK-B", "ALL", "CB")   # Insurance companies
# port_tickers <- c("PAC", "ICTEF", "ATCO-B.ST") # Port operators
```

Future research should expand beyond the current focus:

-   Include adjacent sectors affected by the crisis:
    -   Logistics companies (e.g., freight forwarders, cargo airlines)
    -   Marine insurance providers
    -   Port operators and terminal companies
-   Compare with shipping companies primarily operating in non-affected routes (e.g., Panama Canal, transatlantic)
-   Analyze the spillover effects to commodity markets (oil, consumer goods) and manufacturing supply chains
-   Examine the response of alternative transportation modes (air freight)

## Methodological Enhancements

-   Apply MGARCH models to capture volatility spillovers between shipping and market indices
-   Conduct regime-switching models to better identify structural changes
-   Employ network analysis to visualize changing relationship structures during the crisis
-   Implement textual analysis of shipping company earnings calls and news to correlate market movements with sentiment

These extensions would provide a more comprehensive understanding of the Red Sea crisis's impact on global shipping and financial markets.
